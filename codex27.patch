 (cd "$(git rev-parse --show-toplevel)" && git apply --3way <<'EOF'
diff --git a/config/orchestration/pipelines/pdf-two-phase.yaml b/config/orchestration/pipelines/pdf-two-phase.yaml
index 198c5e9c0b49ba400b880f2fc7ddcfb105def8cf..72931dcec89a50fce06727c5e129809e3b5917a9 100644
--- a/config/orchestration/pipelines/pdf-two-phase.yaml
+++ b/config/orchestration/pipelines/pdf-two-phase.yaml
@@ -1,56 +1,70 @@
 name: pdf-two-phase
 version: "2025-01-01"
 applicable_sources:
   - pmc
   - pmc-fulltext
 metadata:
   owner: ingestion-team
   description: PDF ingestion with MinerU gate.
 stages:
   - name: ingest
     type: ingest
     policy: default
+    config:
+      adapter: openalex
+      parameters:
+        source: pmc
   - name: download
     type: download
     policy: polite-api
     depends_on:
       - ingest
   - name: gate_pdf_ir_ready
     type: gate
     policy: default
     depends_on:
       - download
   - name: chunk
     type: chunk
     policy: gpu-bound
     depends_on:
       - gate_pdf_ir_ready
   - name: embed
     type: embed
     policy: gpu-bound
     depends_on:
       - chunk
   - name: index
     type: index
     policy: default
     depends_on:
       - embed
   - name: extract
     type: extract
     policy: gpu-bound
     depends_on:
       - chunk
   - name: kg
     type: knowledge-graph
     policy: default
     depends_on:
       - extract
       - index
 gates:
   - name: pdf_ir_ready
     resume_stage: chunk
+    timeout_seconds: 1800
+    retry:
+      max_attempts: 3
+      backoff_seconds: 30.0
     condition:
-      field: pdf_ir_ready
-      equals: true
+      logic: all
+      clauses:
+        - field: pdf_ir_ready
+          operator: equals
+          value: true
+        - field: metadata.gate.pdf_ir_ready.status
+          operator: equals
+          value: passed
       timeout_seconds: 900
       poll_interval_seconds: 10.0
diff --git a/docs/guides/orchestration-pipelines.md b/docs/guides/orchestration-pipelines.md
index e70572fedfeb65449f77695d8d7ee4a67637d130..138f02e5320bdbd1f0bbfb4e3eaf5011ee17abc4 100644
--- a/docs/guides/orchestration-pipelines.md
+++ b/docs/guides/orchestration-pipelines.md
@@ -28,53 +28,114 @@ under `Medical_KG_rev.orchestration.dagster` and Haystack components under
 - **Topology YAML** – Pipelines are described in
   `config/orchestration/pipelines/*.yaml`. Each stage lists `name`, `type`,
   optional `policy`, dependencies, and a free-form `config` block. Gates define
   resume conditions, e.g., `pdf_ir_ready=true` for two-phase PDF ingestion.
 - **Resilience policies** – `config/orchestration/resilience.yaml` contains
   shared retry, circuit breaker, and rate limiting definitions. The runtime
   loads these into Tenacity, PyBreaker, and aiolimiter objects.
 - **Version manifest** – `config/orchestration/versions/*` tracks pipeline
   revisions. `PipelineConfigLoader` loads and caches versions to provide
   deterministic orchestration.

 ## Execution Flow

 1. **Job submission** – The gateway builds a `StageContext` and calls
    `submit_to_dagster`. The Dagster run stores the initial state using the job
    ledger resource.
 2. **Stage execution** – Each op resolves the stage implementation via
    `StageFactory`. Resilience policies wrap the execution and emit metrics on
    retries, circuit breaker state changes, and rate limiting delays.
 3. **Ledger updates** – Ops record progress to the job ledger (`current_stage`,
    attempt counts, gate metadata). Sensors poll the ledger for gate conditions
    (e.g., `pdf_ir_ready=true`) and resume downstream stages.
 4. **Outputs** – Stage results are added to the Dagster run state and surfaced
    to the gateway through the ledger/SSE stream. Haystack components persist
    embeddings and metadata in downstream storage systems.
+5. **Phase transitions** – When a gate passes, the runtime records a phase
+   transition, updates the ledger (`gate.<name>.*` metadata), and resumes the
+   downstream graph from the configured `resume_stage`.
+
+### Working with Gates
+
+- **Declaring gates** – Add a `type: gate` stage to the topology and declare a
+  matching entry under `gates`. Each entry requires a `resume_stage`, one or
+  more `clauses`, optional `timeout_seconds`, and optional `retry`
+  configuration. Clauses support `equals`, `exists`, and `changed` operators.
+- **Ledger fields** – Clause `field` values map to `JobLedgerEntry` fields. Use
+  dot-notation for nested metadata (e.g. `metadata.gate.pdf_ir_ready.status`).
+- **Phase tracking** – The runtime groups stages into phases separated by gate
+  stages. Phase metrics are exported via Prometheus (`orchestration_phase_*`).
+- **Resume runs** – The `pdf_ir_ready_sensor` copies `gate_results` and phase
+  data into the run payload, ensuring the resumed Dagster run starts at the
+  correct downstream phase.
+
+#### Example Gate Definition
+
+```yaml
+gates:
+  - name: pdf_ir_ready
+    resume_stage: chunk
+    timeout_seconds: 1800
+    retry:
+      max_attempts: 3
+      backoff_seconds: 30
+    condition:
+      logic: all
+      clauses:
+        - field: pdf_ir_ready
+          operator: equals
+          value: true
+        - field: metadata.gate.pdf_ir_ready.status
+          operator: equals
+          value: passed
+      timeout_seconds: 900
+      poll_interval_seconds: 10
+```
+
+### Gate Tooling
+
+Use `scripts/orchestration_gate_tool.py` to validate and debug gated pipelines:
+
+```bash
+# Validate the pipeline definition and gate wiring
+python scripts/orchestration_gate_tool.py validate pdf-two-phase
+
+# Visualise phase boundaries and stage ordering
+python scripts/orchestration_gate_tool.py visualize pdf-two-phase
+
+# Evaluate a gate against a saved JobLedgerEntry snapshot
+python scripts/orchestration_gate_tool.py evaluate \
+  pdf-two-phase pdf_ir_ready --ledger ledger-entry.json
+```
+
+The script surfaces validation errors (e.g., missing resume stages, invalid
+ledger fields) and prints clause-by-clause evaluation results for rapid
+debugging of two-phase execution issues.

 ## Troubleshooting

 - **Stage resolution errors** – Verify the stage `type` in the topology YAML
   matches the keys registered in `build_default_stage_factory`. Unknown stage
   types raise `StageResolutionError` during job execution.
 - **Resilience misconfiguration** – Check `config/orchestration/resilience.yaml`
   for required fields (attempts, backoff, circuit breaker thresholds). Invalid
   policies raise validation errors at load time.
 - **Gate stalls** – Inspect the job ledger entry to confirm gate metadata is
   set (e.g., `pdf_ir_ready` for PDF pipelines). Sensors poll every ten seconds
-  and record trigger counts in the ledger metadata.
+  and record trigger counts in the ledger metadata. Use the gate tooling script
+  to replay the ledger entry locally and confirm which clause failed.
 - **Missing embeddings** – Ensure the embed stage resolved the Haystack
   embedder; stubs return deterministic values for test runs but do not persist
   to OpenSearch/FAISS.

 ## Operational Notes

 - Run Dagster locally with
   `dagster dev -m Medical_KG_rev.orchestration.dagster.runtime` to access the UI
   and sensors.
 - The gateway uses `StageFactory` directly for synchronous operations (chunking
   and embedding APIs) to avoid spinning up full Dagster runs.
 - Dagster daemon processes handle sensors and schedules. Ensure the daemon has
   access to the same configuration volume as the webserver and gateway.
 - CloudEvents and OpenLineage emission hooks live alongside the Dagster jobs
   and reuse the resilience policy loader for consistent telemetry metadata.

diff --git a/docs/guides/pipeline-schema.json b/docs/guides/pipeline-schema.json
index 9746b1fc564b15827be264c0d4fe7ad6bb70a928..5b515120d80435c8c5fd6449127e3a9b1a0b14a8 100644
--- a/docs/guides/pipeline-schema.json
+++ b/docs/guides/pipeline-schema.json
@@ -1,84 +1,165 @@
 {
   "$defs": {
-    "GateCondition": {
+    "GateConditionClause": {
       "additionalProperties": false,
-      "description": "Predicate evaluated against Job Ledger entries to resume a pipeline.",
+      "description": "Single ledger predicate evaluated during gate execution.",
       "properties": {
-        "equals": {
-          "title": "Equals"
-        },
         "field": {
           "pattern": "^[A-Za-z0-9_.-]+$",
           "title": "Field",
           "type": "string"
         },
+        "operator": {
+          "default": "equals",
+          "enum": [
+            "equals",
+            "exists",
+            "changed"
+          ],
+          "title": "Operator"
+        },
+        "value": {}
+      },
+      "required": [
+        "field"
+      ],
+      "title": "GateConditionClause",
+      "type": "object"
+    },
+    "GateCondition": {
+      "additionalProperties": false,
+      "description": "Predicate evaluated against Job Ledger entries to resume a pipeline.",
+      "properties": {
+        "clauses": {
+          "items": {
+            "$ref": "#/$defs/GateConditionClause"
+          },
+          "minItems": 1,
+          "title": "Clauses",
+          "type": "array"
+        },
+        "logic": {
+          "default": "all",
+          "enum": [
+            "all",
+            "any"
+          ],
+          "title": "Logic"
+        },
         "poll_interval_seconds": {
           "default": 5.0,
           "maximum": 60.0,
           "minimum": 0.5,
           "title": "Poll Interval Seconds",
           "type": "number"
         },
         "timeout_seconds": {
           "anyOf": [
             {
               "maximum": 3600,
               "minimum": 1,
               "type": "integer"
             },
             {
               "type": "null"
             }
           ],
           "default": null,
           "title": "Timeout Seconds"
         }
       },
       "required": [
-        "field",
-        "equals"
+        "clauses"
       ],
       "title": "GateCondition",
       "type": "object"
     },
+    "GateRetryPolicy": {
+      "additionalProperties": false,
+      "description": "Retry configuration for gate polling before timing out.",
+      "properties": {
+        "backoff_seconds": {
+          "default": 0,
+          "maximum": 300,
+          "minimum": 0,
+          "title": "Backoff Seconds",
+          "type": "number"
+        },
+        "max_attempts": {
+          "default": 1,
+          "maximum": 50,
+          "minimum": 1,
+          "title": "Max Attempts",
+          "type": "integer"
+        }
+      },
+      "title": "GateRetryPolicy",
+      "type": "object"
+    },
     "GateDefinition": {
       "additionalProperties": false,
       "description": "Declarative definition for a pipeline gate.",
       "properties": {
         "condition": {
           "$ref": "#/$defs/GateCondition"
         },
         "name": {
           "pattern": "^[A-Za-z0-9_-]+$",
           "title": "Name",
           "type": "string"
         },
         "resume_stage": {
           "pattern": "^[A-Za-z0-9_-]+$",
           "title": "Resume Stage",
           "type": "string"
+        },
+        "retry": {
+          "anyOf": [
+            {
+              "$ref": "#/$defs/GateRetryPolicy"
+            },
+            {
+              "type": "null"
+            }
+          ],
+          "default": null,
+          "title": "Retry"
+        },
+        "timeout_seconds": {
+          "anyOf": [
+            {
+              "maximum": 3600,
+              "minimum": 1,
+              "type": "integer"
+            },
+            {
+              "type": "null"
+            }
+          ],
+          "default": null,
+          "title": "Timeout Seconds"
         }
       },
       "required": [
         "name",
         "condition",
         "resume_stage"
       ],
       "title": "GateDefinition",
       "type": "object"
     },
     "PipelineMetadata": {
       "description": "Optional metadata about the pipeline.",
       "properties": {
         "description": {
           "anyOf": [
             {
               "type": "string"
             },
             {
               "type": "null"
             }
           ],
           "default": null,
           "title": "Description"
         },
@@ -99,50 +180,63 @@
             "type": "string"
           },
           "title": "Tags",
           "type": "array"
         }
       },
       "title": "PipelineMetadata",
       "type": "object"
     },
     "StageDefinition": {
       "additionalProperties": false,
       "description": "Declarative stage specification for topology YAML files.",
       "properties": {
         "config": {
           "additionalProperties": true,
           "title": "Config",
           "type": "object"
         },
         "depends_on": {
           "items": {
             "type": "string"
           },
           "title": "Depends On",
           "type": "array"
         },
+        "gate": {
+          "anyOf": [
+            {
+              "pattern": "^[A-Za-z0-9_-]+$",
+              "type": "string"
+            },
+            {
+              "type": "null"
+            }
+          ],
+          "default": null,
+          "title": "Gate"
+        },
         "name": {
           "pattern": "^[A-Za-z0-9_-]+$",
           "title": "Name",
           "type": "string"
         },
         "policy": {
           "anyOf": [
             {
               "type": "string"
             },
             {
               "type": "null"
             }
           ],
           "default": null,
           "title": "Policy"
         },
         "type": {
           "pattern": "^[A-Za-z0-9_-]+$",
           "title": "Type",
           "type": "string"
         }
       },
       "required": [
         "name",
diff --git a/openspec/changes/add-dagster-gate-support/tasks.md b/openspec/changes/add-dagster-gate-support/tasks.md
index d353234aeea812296a013778c7f6bbe91575a4a4..fcda24c18dfcf78d649bfff0b5b8248634bf6dcf 100644
--- a/openspec/changes/add-dagster-gate-support/tasks.md
+++ b/openspec/changes/add-dagster-gate-support/tasks.md
@@ -1,127 +1,127 @@
 # Implementation Tasks: Dagster Gate Support

 ## 1. Gate Recognition and Classification

 ### 1.1 Gate Detection in Pipeline Building

-- [ ] 1.1.1 Update `_build_pipeline_job` to identify gate stages in topology
-- [ ] 1.1.2 Separate stages into pre-gate and post-gate phases
-- [ ] 1.1.3 Create dependency graph that respects gate boundaries
-- [ ] 1.1.4 Add validation that gates have no output dependencies
+- [x] 1.1.1 Update `_build_pipeline_job` to identify gate stages in topology
+- [x] 1.1.2 Separate stages into pre-gate and post-gate phases
+- [x] 1.1.3 Create dependency graph that respects gate boundaries
+- [x] 1.1.4 Add validation that gates have no output dependencies

 ### 1.2 Gate Metadata and Configuration

-- [ ] 1.2.1 Extend `StageDefinition` to include gate-specific metadata
-- [ ] 1.2.2 Define gate condition schema (ledger field checks, operators, values)
-- [ ] 1.2.3 Add gate timeout and retry configuration options
-- [ ] 1.2.4 Create gate condition evaluator class
+- [x] 1.2.1 Extend `StageDefinition` to include gate-specific metadata
+- [x] 1.2.2 Define gate condition schema (ledger field checks, operators, values)
+- [x] 1.2.3 Add gate timeout and retry configuration options
+- [x] 1.2.4 Create gate condition evaluator class

 ## 2. Two-Phase Execution Architecture

 ### 2.1 Phase-Based Job Construction

-- [ ] 2.1.1 Create separate Dagster graphs for each execution phase
-- [ ] 2.1.2 Implement phase transition logic with gate evaluation
-- [ ] 2.1.3 Add phase state tracking in job execution context
-- [ ] 2.1.4 Handle phase failures and rollbacks appropriately
+- [x] 2.1.1 Create separate Dagster graphs for each execution phase
+- [x] 2.1.2 Implement phase transition logic with gate evaluation
+- [x] 2.1.3 Add phase state tracking in job execution context
+- [x] 2.1.4 Handle phase failures and rollbacks appropriately

 ### 2.2 Gate Execution Implementation

-- [ ] 2.2.1 Create `GateStage` class that evaluates conditions without producing outputs
-- [ ] 2.2.2 Implement ledger-based condition checking
-- [ ] 2.2.3 Add `GateConditionError` for failed gate evaluations
-- [ ] 2.2.4 Support multiple condition types (field exists, field equals, field changed)
+- [x] 2.2.1 Create `GateStage` class that evaluates conditions without producing outputs
+- [x] 2.2.2 Implement ledger-based condition checking
+- [x] 2.2.3 Add `GateConditionError` for failed gate evaluations
+- [x] 2.2.4 Support multiple condition types (field exists, field equals, field changed)

 ### 2.3 Enhanced State Management

-- [ ] 2.3.1 Update `_apply_stage_output` to handle gate stages (no state changes)
-- [ ] 2.3.2 Add gate evaluation results to execution state
-- [ ] 2.3.3 Track gate success/failure in job metadata
-- [ ] 2.3.4 Implement gate timeout handling and state cleanup
+- [x] 2.3.1 Update `_apply_stage_output` to handle gate stages (no state changes)
+- [x] 2.3.2 Add gate evaluation results to execution state
+- [x] 2.3.3 Track gate success/failure in job metadata
+- [x] 2.3.4 Implement gate timeout handling and state cleanup

 ## 3. Sensor Integration for Resumption

 ### 3.1 Resume Job Creation

-- [ ] 3.1.1 Modify `pdf_ir_ready_sensor` to create resume jobs correctly
-- [ ] 3.1.2 Implement proper phase targeting for resume execution
-- [ ] 3.1.3 Add resume job validation and error handling
-- [ ] 3.1.4 Connect resume jobs to original job context
+- [x] 3.1.1 Modify `pdf_ir_ready_sensor` to create resume jobs correctly
+- [x] 3.1.2 Implement proper phase targeting for resume execution
+- [x] 3.1.3 Add resume job validation and error handling
+- [x] 3.1.4 Connect resume jobs to original job context

 ### 3.2 Cross-Phase State Management

-- [ ] 3.2.1 Ensure resume jobs inherit state from original execution
-- [ ] 3.2.2 Handle state serialization for job persistence
-- [ ] 3.2.3 Implement state validation for resume operations
-- [ ] 3.2.4 Add state cleanup for completed or failed jobs
+- [x] 3.2.1 Ensure resume jobs inherit state from original execution
+- [x] 3.2.2 Handle state serialization for job persistence
+- [x] 3.2.3 Implement state validation for resume operations
+- [x] 3.2.4 Add state cleanup for completed or failed jobs

 ## 4. Pipeline Schema Enhancements

 ### 4.1 Gate Definition Schema

-- [ ] 4.1.1 Extend `PipelineTopologyConfig` to include gate definitions
-- [ ] 4.1.2 Define `GateDefinition` with condition, timeout, and resume_stage
-- [ ] 4.1.3 Add gate validation in pipeline loading
-- [ ] 4.1.4 Support multiple gates per pipeline
+- [x] 4.1.1 Extend `PipelineTopologyConfig` to include gate definitions
+- [x] 4.1.2 Define `GateDefinition` with condition, timeout, and resume_stage
+- [x] 4.1.3 Add gate validation in pipeline loading
+- [x] 4.1.4 Support multiple gates per pipeline

 ### 4.2 Enhanced Pipeline Validation

-- [ ] 4.2.1 Validate gate conditions reference valid ledger fields
-- [ ] 4.2.2 Check that resume stages exist and are post-gate
-- [ ] 4.2.3 Ensure gates don't have output-producing dependencies
-- [ ] 4.2.4 Validate timeout values are reasonable
+- [x] 4.2.1 Validate gate conditions reference valid ledger fields
+- [x] 4.2.2 Check that resume stages exist and are post-gate
+- [x] 4.2.3 Ensure gates don't have output-producing dependencies
+- [x] 4.2.4 Validate timeout values are reasonable

 ## 5. Testing and Validation

 ### 5.1 Unit Tests for Gate Logic

-- [ ] 5.1.1 Test gate condition evaluation with various ledger states
-- [ ] 5.1.2 Test gate timeout and error handling
-- [ ] 5.1.3 Test gate stage execution (no output production)
-- [ ] 5.1.4 Test gate metadata validation
+- [x] 5.1.1 Test gate condition evaluation with various ledger states
+- [x] 5.1.2 Test gate timeout and error handling
+- [x] 5.1.3 Test gate stage execution (no output production)
+- [x] 5.1.4 Test gate metadata validation

 ### 5.2 Integration Tests for Two-Phase Execution

-- [ ] 5.2.1 Test complete two-phase pipeline execution
-- [ ] 5.2.2 Test gate failure scenarios and error propagation
-- [ ] 5.2.3 Test sensor-based job resumption
-- [ ] 5.2.4 Test state management across execution phases
+- [x] 5.2.1 Test complete two-phase pipeline execution
+- [x] 5.2.2 Test gate failure scenarios and error propagation
+- [x] 5.2.3 Test sensor-based job resumption
+- [x] 5.2.4 Test state management across execution phases

 ### 5.3 Pipeline Validation Tests

-- [ ] 5.3.1 Test pipeline loading with gate definitions
-- [ ] 5.3.2 Test invalid gate configurations are rejected
-- [ ] 5.3.3 Test dependency validation for gated pipelines
-- [ ] 5.3.4 Test pipeline serialization and deserialization
+- [x] 5.3.1 Test pipeline loading with gate definitions
+- [x] 5.3.2 Test invalid gate configurations are rejected
+- [x] 5.3.3 Test dependency validation for gated pipelines
+- [x] 5.3.4 Test pipeline serialization and deserialization

 ## 6. Documentation and Monitoring

 ### 6.1 Enhanced Pipeline Documentation

-- [ ] 6.1.1 Update `docs/guides/dagster-orchestration.md` with gate examples
-- [ ] 6.1.2 Document gate condition syntax and operators
-- [ ] 6.1.3 Add troubleshooting guide for gate failures
-- [ ] 6.1.4 Document two-phase execution model
+- [x] 6.1.1 Update `docs/guides/dagster-orchestration.md` with gate examples
+- [x] 6.1.2 Document gate condition syntax and operators
+- [x] 6.1.3 Add troubleshooting guide for gate failures
+- [x] 6.1.4 Document two-phase execution model

 ### 6.2 Monitoring and Observability

-- [ ] 6.2.1 Add metrics for gate evaluation success/failure rates
-- [ ] 6.2.2 Track execution phase transitions
-- [ ] 6.2.3 Monitor gate timeout occurrences
-- [ ] 6.2.4 Add structured logging for gate operations
+- [x] 6.2.1 Add metrics for gate evaluation success/failure rates
+- [x] 6.2.2 Track execution phase transitions
+- [x] 6.2.3 Monitor gate timeout occurrences
+- [x] 6.2.4 Add structured logging for gate operations

 ### 6.3 Developer Tools

-- [ ] 6.3.1 Create pipeline validation CLI tool
-- [ ] 6.3.2 Add gate condition testing utilities
-- [ ] 6.3.3 Implement pipeline visualization with gate flow
-- [ ] 6.3.4 Add debugging tools for gate evaluation
+- [x] 6.3.1 Create pipeline validation CLI tool
+- [x] 6.3.2 Add gate condition testing utilities
+- [x] 6.3.3 Implement pipeline visualization with gate flow
+- [x] 6.3.4 Add debugging tools for gate evaluation

 **Total Tasks**: 45 across 6 work streams

 **Risk Assessment:**

 - **Medium Risk**: Changes to core execution logic could affect pipeline reliability
 - **Low Risk**: Gate functionality is additive and doesn't break existing pipelines

 **Rollback Plan**: If issues arise, disable gate processing and fall back to linear execution while keeping gate definitions for future use.
diff --git a/scripts/orchestration_gate_tool.py b/scripts/orchestration_gate_tool.py
new file mode 100644
index 0000000000000000000000000000000000000000..761590d3e9248abd81ec721726a9a03187443e6e
--- /dev/null
+++ b/scripts/orchestration_gate_tool.py
@@ -0,0 +1,224 @@
+"""CLI helpers for inspecting gated Dagster pipelines."""
+
+from __future__ import annotations
+
+import argparse
+import json
+import sys
+from collections import defaultdict
+from pathlib import Path
+from typing import Any, Iterable
+
+from Medical_KG_rev.orchestration.dagster.configuration import (
+    GateDefinition,
+    PipelineConfigLoader,
+    PipelineTopologyConfig,
+)
+from Medical_KG_rev.orchestration.dagster.runtime import GateConditionEvaluator
+from Medical_KG_rev.orchestration.ledger import JobLedgerEntry
+from Medical_KG_rev.utils.logging import get_logger
+
+logger = get_logger(__name__)
+
+
+def _topological_order(stages: Iterable[tuple[str, list[str]]]) -> list[str]:
+    graph: dict[str, set[str]] = {name: set(deps) for name, deps in stages}
+    resolved: list[str] = []
+    temporary: set[str] = set()
+    permanent: set[str] = set()
+
+    def visit(node: str) -> None:
+        if node in permanent:
+            return
+        if node in temporary:
+            raise ValueError(f"cycle detected involving stage '{node}'")
+        temporary.add(node)
+        for dep in graph.get(node, set()):
+            if dep in graph:
+                visit(dep)
+        temporary.remove(node)
+        permanent.add(node)
+        resolved.append(node)
+
+    for node in graph:
+        visit(node)
+    return resolved
+
+
+def _compute_phases(topology: PipelineTopologyConfig) -> dict[str, int]:
+    stage_pairs = [(stage.name, stage.depends_on) for stage in topology.stages]
+    order = _topological_order(stage_pairs)
+    lookup = {stage.name: stage for stage in topology.stages}
+    phase_index = 1
+    phase_map: dict[str, int] = {}
+    for name in order:
+        stage = lookup[name]
+        phase_map[name] = phase_index
+        if stage.stage_type == "gate":
+            phase_index += 1
+    return phase_map
+
+
+def _load_pipeline(base_path: Path, name: str) -> PipelineTopologyConfig:
+    loader = PipelineConfigLoader(base_path)
+    return loader.load(name)
+
+
+def _print(msg: str) -> None:
+    sys.stdout.write(msg + "\n")
+
+
+def _phase_summary(topology: PipelineTopologyConfig) -> str:
+    phase_map = _compute_phases(topology)
+    buckets: dict[int, list[str]] = defaultdict(list)
+    for stage in topology.stages:
+        buckets[phase_map[stage.name]].append(stage.name)
+    lines = [f"Pipeline: {topology.name} (version {topology.version})"]
+    for phase in sorted(buckets):
+        lines.append(f"  Phase {phase}:")
+        for stage_name in buckets[phase]:
+            stage = next(item for item in topology.stages if item.name == stage_name)
+            marker = " [gate]" if stage.stage_type == "gate" else ""
+            deps = f" (depends on {', '.join(stage.depends_on)})" if stage.depends_on else ""
+            lines.append(f"    - {stage_name} [{stage.stage_type}]{marker}{deps}")
+    if topology.gates:
+        lines.append("  Gates:")
+        for gate in topology.gates:
+            clause_parts = []
+            for clause in gate.condition.clauses:
+                value = json.dumps(clause.value)
+                clause_parts.append(f"{clause.field} {clause.operator.value} {value}")
+            lines.append(
+                "    - {name}: resume='{resume}' logic={logic} clauses=[{clauses}]".format(
+                    name=gate.name,
+                    resume=gate.resume_stage,
+                    logic=gate.condition.logic,
+                    clauses="; ".join(clause_parts),
+                )
+            )
+    return "\n".join(lines)
+
+
+def _load_json(path: str) -> dict[str, Any]:
+    if path == "-":
+        payload = sys.stdin.read()
+    else:
+        payload = Path(path).read_text()
+    return json.loads(payload)
+
+
+def _build_entry(data: dict[str, Any]) -> JobLedgerEntry:
+    required = {"job_id", "doc_key", "tenant_id"}
+    missing = [field for field in required if field not in data]
+    if missing:
+        raise ValueError(f"ledger entry missing required fields: {', '.join(missing)}")
+    allowed = set(JobLedgerEntry.__annotations__.keys())
+    skip = {"created_at", "updated_at", "completed_at"}
+    kwargs = {key: data[key] for key in allowed if key in data and key not in skip}
+    entry = JobLedgerEntry(**kwargs)
+    if "metadata" in data and not isinstance(entry.metadata, dict):
+        entry.metadata = dict(data["metadata"])
+    return entry
+
+
+def _describe_gate(gate: GateDefinition) -> str:
+    lines = [f"Gate '{gate.name}' → resume '{gate.resume_stage}'"]
+    lines.append(f"  logic: {gate.condition.logic}")
+    if gate.timeout_seconds:
+        lines.append(f"  timeout_seconds: {gate.timeout_seconds}")
+    if gate.retry:
+        lines.append(
+            f"  retry: attempts={gate.retry.max_attempts} backoff={gate.retry.backoff_seconds}s"
+        )
+    for clause in gate.condition.clauses:
+        lines.append(
+            f"  clause: field='{clause.field}' operator={clause.operator.value} value={clause.value!r}"
+        )
+    return "\n".join(lines)
+
+
+def cmd_validate(args: argparse.Namespace) -> int:
+    try:
+        topology = _load_pipeline(args.base_path, args.pipeline)
+    except Exception as exc:  # pragma: no cover - CLI surface
+        logger.error("gate_tool.validate.error", pipeline=args.pipeline, error=str(exc))
+        _print(f"Validation failed: {exc}")
+        return 1
+    _print("Validation succeeded.")
+    _print(_phase_summary(topology))
+    return 0
+
+
+def cmd_visualize(args: argparse.Namespace) -> int:
+    topology = _load_pipeline(args.base_path, args.pipeline)
+    _print(_phase_summary(topology))
+    return 0
+
+
+def cmd_evaluate(args: argparse.Namespace) -> int:
+    topology = _load_pipeline(args.base_path, args.pipeline)
+    gate = next((item for item in topology.gates if item.name == args.gate), None)
+    if gate is None:
+        _print(f"Gate '{args.gate}' not found in pipeline '{args.pipeline}'.")
+        return 1
+    entry_payload = _load_json(args.ledger)
+    previous_payload = _load_json(args.previous) if args.previous else {}
+    try:
+        entry = _build_entry(entry_payload)
+    except ValueError as exc:
+        _print(f"Invalid ledger entry: {exc}")
+        return 1
+    evaluator = GateConditionEvaluator(gate)
+    satisfied, observed = evaluator.evaluate(entry, previous_observed=previous_payload)
+    _print(_describe_gate(gate))
+    _print("Observed values:")
+    for field, value in observed.items():
+        _print(f"  {field}: {value!r}")
+    _print(f"Gate result: {'PASSED' if satisfied else 'BLOCKED'}")
+    return 0 if satisfied or args.allow_failure else 2
+
+
+def build_parser() -> argparse.ArgumentParser:
+    parser = argparse.ArgumentParser(description="Dagster gate utilities")
+    parser.add_argument(
+        "--base-path",
+        type=Path,
+        default=Path("config/orchestration/pipelines"),
+        help="Pipeline topology directory",
+    )
+    subparsers = parser.add_subparsers(dest="command", required=True)
+
+    validate_parser = subparsers.add_parser("validate", help="Validate pipeline configuration")
+    validate_parser.add_argument("pipeline", help="Pipeline name (YAML stem)")
+    validate_parser.set_defaults(func=cmd_validate)
+
+    viz_parser = subparsers.add_parser("visualize", help="Visualize phase ordering")
+    viz_parser.add_argument("pipeline", help="Pipeline name (YAML stem)")
+    viz_parser.set_defaults(func=cmd_visualize)
+
+    eval_parser = subparsers.add_parser("evaluate", help="Evaluate a gate against a ledger entry")
+    eval_parser.add_argument("pipeline", help="Pipeline name (YAML stem)")
+    eval_parser.add_argument("gate", help="Gate identifier")
+    eval_parser.add_argument("--ledger", required=True, help="Path to ledger entry JSON (use '-' for stdin)")
+    eval_parser.add_argument(
+        "--previous",
+        help="Optional JSON with previous observed values for 'changed' clauses",
+    )
+    eval_parser.add_argument(
+        "--allow-failure",
+        action="store_true",
+        help="Return success even if the gate conditions are not satisfied",
+    )
+    eval_parser.set_defaults(func=cmd_evaluate)
+
+    return parser
+
+
+def main(argv: list[str] | None = None) -> int:
+    parser = build_parser()
+    args = parser.parse_args(argv)
+    return args.func(args)
+
+
+if __name__ == "__main__":
+    raise SystemExit(main())
diff --git a/src/Medical_KG_rev/observability/metrics.py b/src/Medical_KG_rev/observability/metrics.py
index fb176cbaa019671f5694d857f620845a814bcfa1..5205eaac2e37c11ee9e853849aa0686eb482206e 100644
--- a/src/Medical_KG_rev/observability/metrics.py
+++ b/src/Medical_KG_rev/observability/metrics.py
@@ -99,50 +99,77 @@ CHUNKING_DOCUMENTS = Counter(
 CHUNKING_DURATION = Histogram(
     "chunking_duration_seconds",
     "Chunking duration distribution per profile",
     labelnames=("profile",),
     buckets=(0.05, 0.1, 0.25, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0),
 )
 CHUNKS_PER_DOCUMENT = Histogram(
     "chunking_chunks_per_document",
     "Distribution of chunk counts per document",
     labelnames=("profile",),
     buckets=(1, 2, 4, 8, 16, 32, 64, 128),
 )
 CHUNKING_FAILURES = Counter(
     "medicalkg_chunking_errors_total",
     "Chunking failures grouped by profile and error type",
     labelnames=("profile", "error_type"),
 )
 MINERU_GATE_TRIGGERED = Counter(
     "mineru_gate_triggered_total",
     "Number of times the MinerU two-phase gate halted processing",
 )
 POSTPDF_START_TRIGGERED = Counter(
     "postpdf_start_triggered_total",
     "Number of times post-PDF resume was triggered",
 )
+GATE_EVALUATIONS = Counter(
+    "orchestration_gate_evaluations_total",
+    "Gate evaluation outcomes grouped by gate and result",
+    labelnames=("gate", "result"),
+)
+GATE_DURATION = Histogram(
+    "orchestration_gate_duration_seconds",
+    "Gate evaluation duration distribution",
+    labelnames=("gate",),
+    buckets=(1.0, 5.0, 15.0, 30.0, 60.0, 120.0, 300.0),
+)
+GATE_ATTEMPTS = Histogram(
+    "orchestration_gate_attempts",
+    "Number of polling attempts per gate evaluation",
+    labelnames=("gate",),
+    buckets=(1, 2, 3, 5, 8, 13, 21),
+)
+GATE_TIMEOUTS = Counter(
+    "orchestration_gate_timeouts_total",
+    "Number of times a gate evaluation exceeded its timeout",
+    labelnames=("gate",),
+)
+PHASE_TRANSITIONS = Counter(
+    "orchestration_phase_transitions_total",
+    "Phase transitions observed during gated pipeline execution",
+    labelnames=("pipeline", "phase"),
+)
 CHUNKING_CIRCUIT_STATE = Gauge(
     "chunking_circuit_breaker_state",
     "Circuit breaker state for chunking pipeline (0=closed, 1=open, 2=half-open)",
 )
 GPU_UTILISATION = Gauge(
     "gpu_utilization_percent",
     "GPU memory utilisation percentage",
     labelnames=("gpu",),
 )
 BUSINESS_EVENTS = Counter(
     "business_events",
     "Business event counters (documents ingested, retrievals)",
     labelnames=("event",),
 )
 JOB_STATUS_COUNTS = Gauge(
     "job_status_counts",
     "Current count of jobs by status",
     labelnames=("status",),
 )
 RERANK_OPERATIONS = Counter(
     "reranking_operations_total",
     "Total reranking invocations",
     labelnames=("reranker", "tenant", "batch_size"),
 )
 RERANK_DURATION = Histogram(
@@ -292,50 +319,70 @@ def register_metrics(app: FastAPI, settings: AppSettings) -> None:  # type: igno

     @app.get(path, include_in_schema=False)
     async def metrics_endpoint() -> "Response":
         return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)


 def record_resilience_retry(policy: str, stage: str) -> None:
     """Increment retry counter for the supplied policy and stage."""

     RESILIENCE_RETRY_ATTEMPTS.labels(policy, stage).inc()


 def record_resilience_circuit_state(policy: str, stage: str, state: str) -> None:
     """Update gauge with the numeric circuit breaker state."""

     mapping = {"closed": 0.0, "open": 1.0, "half-open": 2.0}
     RESILIENCE_CIRCUIT_STATE.labels(policy, stage).set(mapping.get(state.lower(), -1.0))


 def record_resilience_rate_limit_wait(policy: str, stage: str, wait_seconds: float) -> None:
     """Observe rate limit wait duration."""

     RESILIENCE_RATE_LIMIT_WAIT.labels(policy, stage).observe(wait_seconds)


+def record_gate_evaluation(gate: str, result: str, duration: float, *, attempts: int) -> None:
+    """Publish metrics for a gate evaluation attempt."""
+
+    _increment_with_exemplar(GATE_EVALUATIONS, (gate, result))
+    _observe_with_exemplar(GATE_DURATION, (gate,), duration)
+    _observe_with_exemplar(GATE_ATTEMPTS, (gate,), float(max(attempts, 0)))
+
+
+def record_gate_timeout(gate: str) -> None:
+    """Increment timeout counter for the specified gate."""
+
+    _increment_with_exemplar(GATE_TIMEOUTS, (gate,))
+
+
+def record_phase_transition(pipeline: str, phase: str) -> None:
+    """Track transitions between orchestration phases."""
+
+    _increment_with_exemplar(PHASE_TRANSITIONS, (pipeline, phase))
+
+
 def _observe_with_exemplar(metric, labels: tuple[str, ...], value: float) -> None:
     labelled = metric.labels(*labels)
     correlation_id = get_correlation_id()
     kwargs: dict[str, object] = {}
     if correlation_id:
         try:  # pragma: no cover - exemplar support optional
             kwargs["exemplar"] = {"correlation_id": correlation_id}
         except TypeError:
             kwargs = {}
     labelled.observe(max(value, 0.0), **kwargs)


 def _increment_with_exemplar(metric, labels: tuple[str, ...], amount: float = 1.0) -> None:
     labelled = metric.labels(*labels)
     correlation_id = get_correlation_id()
     kwargs: dict[str, object] = {}
     if correlation_id:
         try:  # pragma: no cover - exemplar support optional
             kwargs["exemplar"] = {"correlation_id": correlation_id}
         except TypeError:
             kwargs = {}
     labelled.inc(amount, **kwargs)


 def observe_job_duration(operation: str, duration_seconds: float) -> None:
diff --git a/src/Medical_KG_rev/orchestration/dagster/configuration.py b/src/Medical_KG_rev/orchestration/dagster/configuration.py
index 327e9350a8873029eae230c73a893f09854f95d9..e5e7889b5a53e034494e3d9e59e00086b16d7b8f 100644
--- a/src/Medical_KG_rev/orchestration/dagster/configuration.py
+++ b/src/Medical_KG_rev/orchestration/dagster/configuration.py
@@ -1,101 +1,193 @@
 """Configuration models and loaders for Dagster-based orchestration."""

 from __future__ import annotations

 import asyncio
 import json
 import threading
 import time
 from collections.abc import Callable, Iterable
 from dataclasses import dataclass
 from enum import Enum
 from pathlib import Path
-from typing import TYPE_CHECKING, Any, Callable, Mapping
+from typing import TYPE_CHECKING, Any, Callable, Iterable, Mapping, Sequence
+from typing import Literal

 import yaml
 from pydantic import (
     BaseModel,
     ConfigDict,
     Field,
     PrivateAttr,
     ValidationError,
     field_validator,
     model_validator,
 )

 from Medical_KG_rev.observability.metrics import (
     record_resilience_circuit_state,
     record_resilience_rate_limit_wait,
     record_resilience_retry,
 )
+from Medical_KG_rev.orchestration.ledger import JobLedgerEntry
 from Medical_KG_rev.utils.logging import get_logger

 logger = get_logger(__name__)

 if TYPE_CHECKING:  # pragma: no cover - hints only
     from aiolimiter import AsyncLimiter
     from pybreaker import CircuitBreaker


 class BackoffStrategy(str, Enum):
     EXPONENTIAL = "exponential"
     LINEAR = "linear"
     NONE = "none"


+class GateConditionOperator(str, Enum):
+    """Supported logical operators for gate predicates."""
+
+    EQUALS = "equals"
+    EXISTS = "exists"
+    CHANGED = "changed"
+
+
+class GateConditionClause(BaseModel):
+    """Single ledger predicate evaluated during gate execution."""
+
+    model_config = ConfigDict(extra="forbid")
+
+    field: str = Field(pattern=r"^[A-Za-z0-9_.-]+$")
+    operator: GateConditionOperator = Field(default=GateConditionOperator.EQUALS)
+    value: Any | None = Field(default=None)
+
+    @model_validator(mode="after")
+    def _validate_value(self) -> "GateConditionClause":
+        if self.operator is GateConditionOperator.EQUALS and self.value is None:
+            raise ValueError("equals operator requires a value")
+        if self.operator is GateConditionOperator.EXISTS and self.value not in (None, True):
+            raise ValueError("exists operator does not accept a comparison value")
+        return self
+
+
 class GateCondition(BaseModel):
     """Predicate evaluated against Job Ledger entries to resume a pipeline."""

     model_config = ConfigDict(extra="forbid")

-    field: str = Field(pattern=r"^[A-Za-z0-9_.-]+$")
-    equals: Any
+    clauses: list[GateConditionClause]
+    logic: Literal["all", "any"] = Field(default="all")
     timeout_seconds: int | None = Field(default=None, ge=1, le=3600)
     poll_interval_seconds: float = Field(default=5.0, ge=0.5, le=60.0)

+    @model_validator(mode="before")
+    @classmethod
+    def _normalise_legacy(cls, data: Mapping[str, Any] | Any) -> Mapping[str, Any]:
+        if not isinstance(data, Mapping):
+            return data
+        if "clauses" in data:
+            return data
+        clauses: list[Mapping[str, Any]] = []
+        logic = data.get("logic", "all")
+        if "all_of" in data or "any_of" in data:
+            if "all_of" in data:
+                clauses = list(data["all_of"])
+                logic = "all"
+            if "any_of" in data:
+                clauses = list(data["any_of"])
+                logic = "any"
+        elif "field" in data:
+            clause = {
+                "field": data["field"],
+                "operator": data.get("operator", data.get("op", "equals")),
+                "value": data.get("equals", data.get("value")),
+            }
+            clauses = [clause]
+        normalised: dict[str, Any] = dict(data)
+        normalised.pop("field", None)
+        normalised.pop("equals", None)
+        normalised.pop("operator", None)
+        normalised.pop("op", None)
+        normalised.pop("value", None)
+        if "all_of" in normalised:
+            normalised.pop("all_of")
+        if "any_of" in normalised:
+            normalised.pop("any_of")
+        normalised["clauses"] = clauses
+        normalised["logic"] = logic
+        return normalised
+
+    @field_validator("clauses")
+    @classmethod
+    def _require_clauses(cls, value: Sequence[GateConditionClause]) -> list[GateConditionClause]:
+        if not value:
+            raise ValueError("GateCondition requires at least one clause")
+        return list(value)
+
+
+class GateRetryPolicy(BaseModel):
+    """Retry configuration for gate polling before timing out."""
+
+    model_config = ConfigDict(extra="forbid")
+
+    max_attempts: int = Field(default=1, ge=1, le=50)
+    backoff_seconds: float = Field(default=0.0, ge=0.0, le=300.0)
+

 class GateDefinition(BaseModel):
     """Declarative definition for a pipeline gate."""

     model_config = ConfigDict(extra="forbid")

     name: str = Field(pattern=r"^[A-Za-z0-9_-]+$")
     condition: GateCondition
     resume_stage: str = Field(pattern=r"^[A-Za-z0-9_-]+$")
+    timeout_seconds: int | None = Field(default=None, ge=1, le=3600)
+    retry: GateRetryPolicy | None = None


 class StageDefinition(BaseModel):
     """Declarative stage specification for topology YAML files."""

     model_config = ConfigDict(extra="forbid", populate_by_name=True)

     name: str = Field(pattern=r"^[A-Za-z0-9_-]+$")
     stage_type: str = Field(alias="type", pattern=r"^[A-Za-z0-9_-]+$")
     policy: str | None = Field(default=None, alias="policy")
     depends_on: list[str] = Field(default_factory=list, alias="depends_on")
     config: dict[str, Any] = Field(default_factory=dict)
+    gate_name: str | None = Field(default=None, alias="gate")
+
+    @model_validator(mode="after")
+    def _normalise_gate(self) -> "StageDefinition":
+        if self.stage_type == "gate" and not self.gate_name:
+            self.gate_name = self.name
+        if self.stage_type != "gate" and self.gate_name:
+            raise ValueError("gate metadata can only be set on gate stage types")
+        return self

     @field_validator("depends_on")
     @classmethod
     def _unique_dependencies(cls, value: Iterable[str]) -> list[str]:
         seen: set[str] = set()
         result: list[str] = []
         for item in value:
             if item in seen:
                 raise ValueError(f"duplicate dependency '{item}' declared for stage")
             seen.add(item)
             result.append(item)
         return result


 class PipelineMetadata(BaseModel):
     """Optional metadata about the pipeline."""

     owner: str | None = None
     description: str | None = None
     tags: list[str] = Field(default_factory=list)


 class PipelineTopologyConfig(BaseModel):
     """Complete topology definition for a pipeline."""

@@ -105,56 +197,107 @@ class PipelineTopologyConfig(BaseModel):
     version: str = Field(pattern=r"^[0-9]{4}-[0-9]{2}-[0-9]{2}(-[A-Za-z0-9]+)?$")
     applicable_sources: list[str] = Field(default_factory=list)
     stages: list[StageDefinition]
     gates: list[GateDefinition] = Field(default_factory=list)
     metadata: PipelineMetadata | None = None

     @model_validator(mode="after")
     def _validate_dependencies(self) -> PipelineTopologyConfig:
         stage_names = [stage.name for stage in self.stages]
         if len(stage_names) != len(set(stage_names)):
             duplicates = {name for name in stage_names if stage_names.count(name) > 1}
             raise ValueError(f"duplicate stage names detected: {sorted(duplicates)}")

         stage_set = set(stage_names)
         for stage in self.stages:
             missing = [dep for dep in stage.depends_on if dep not in stage_set]
             if missing:
                 raise ValueError(
                     f"stage '{stage.name}' declares unknown dependencies: {', '.join(sorted(missing))}"
                 )

         order = _topological_sort({stage.name: stage.depends_on for stage in self.stages})
         if order is None:
             raise ValueError("cycle detected in pipeline dependencies")

-        gate_stage_set = {stage.name for stage in self.stages}
+        stage_lookup = {stage.name: stage for stage in self.stages}
+        order_index = {name: index for index, name in enumerate(order)}
+        gate_stage_by_name = {
+            stage.gate_name or stage.name: stage
+            for stage in self.stages
+            if stage.stage_type == "gate"
+        }
+        dependents: dict[str, list[str]] = {}
+        for candidate in self.stages:
+            for dep in candidate.depends_on:
+                dependents.setdefault(dep, []).append(candidate.name)
+
         for gate in self.gates:
-            if gate.resume_stage not in gate_stage_set:
+            if gate.name not in gate_stage_by_name:
+                raise ValueError(f"gate definition '{gate.name}' does not match a gate stage")
+            if gate.resume_stage not in stage_lookup:
                 raise ValueError(
                     f"gate '{gate.name}' references unknown resume_stage '{gate.resume_stage}'"
                 )
+            gate_stage = gate_stage_by_name[gate.name]
+            if order_index[gate.resume_stage] <= order_index[gate_stage.name]:
+                raise ValueError(
+                    f"gate '{gate.name}' resume_stage '{gate.resume_stage}' must be downstream of gate stage"
+                )
+            resume_dependencies = stage_lookup[gate.resume_stage].depends_on
+            if gate_stage.name not in resume_dependencies:
+                raise ValueError(
+                    f"resume_stage '{gate.resume_stage}' must depend on gate stage '{gate_stage.name}'"
+                )
+            invalid_dependents = [
+                name for name in dependents.get(gate_stage.name, []) if name != gate.resume_stage
+            ]
+            if invalid_dependents:
+                raise ValueError(
+                    f"gate stage '{gate_stage.name}' may only be depended on by resume_stage "
+                    f"'{gate.resume_stage}'; found: {', '.join(sorted(invalid_dependents))}"
+                )
+            if gate.timeout_seconds and gate.condition.timeout_seconds:
+                if gate.timeout_seconds < gate.condition.timeout_seconds:
+                    raise ValueError(
+                        "gate timeout_seconds must exceed or equal condition timeout"
+                    )
+            for dep in resume_dependencies:
+                if dep == gate_stage.name:
+                    continue
+                upstream = stage_lookup.get(dep)
+                if upstream and upstream.stage_type == "gate":
+                    raise ValueError(
+                        f"gate '{gate.name}' resume_stage '{gate.resume_stage}' cannot depend on multiple gates"
+                    )
+            valid_fields = set(JobLedgerEntry.__annotations__)
+            for clause in gate.condition.clauses:
+                root_field = clause.field.split(".", 1)[0]
+                if root_field not in valid_fields:
+                    raise ValueError(
+                        f"gate '{gate.name}' references unknown ledger field '{root_field}'"
+                    )
         return self


 class CircuitBreakerConfig(BaseModel):
     failure_threshold: int = Field(ge=3, le=10)
     recovery_timeout: float = Field(ge=1.0, le=600.0)
     expected_exception: str | None = None


 class RateLimitConfig(BaseModel):
     rate_limit_per_second: float = Field(ge=0.1, le=100.0)


 class BackoffConfig(BaseModel):
     strategy: BackoffStrategy = Field(default=BackoffStrategy.EXPONENTIAL)
     initial: float = Field(default=0.5, ge=0.0, le=60.0)
     maximum: float = Field(default=30.0, ge=0.0, le=600.0)
     jitter: bool = Field(default=True)

     @model_validator(mode="after")
     def _validate_bounds(self) -> BackoffConfig:
         if self.strategy is BackoffStrategy.NONE:
             return self
         if self.initial < 0.05:
             raise ValueError("initial backoff must be >=0.05 for non-none strategies")
diff --git a/src/Medical_KG_rev/orchestration/dagster/runtime.py b/src/Medical_KG_rev/orchestration/dagster/runtime.py
index 11d5dc449d8439644b6964dddca4df13385d11d3..8cfd9db5b9fa68865d995b6c6436dae35fac287a 100644
--- a/src/Medical_KG_rev/orchestration/dagster/runtime.py
+++ b/src/Medical_KG_rev/orchestration/dagster/runtime.py
@@ -4,99 +4,282 @@ from __future__ import annotations

 from dataclasses import dataclass
 import re
 import time
 from pathlib import Path
 from typing import Any, Callable, Mapping, Sequence
 from uuid import uuid4

 from dagster import (
     Definitions,
     ExecuteInProcessResult,
     In,
     Out,
     ResourceDefinition,
     RunRequest,
     SensorEvaluationContext,
     SkipReason,
     graph,
     op,
     sensor,
 )

 from Medical_KG_rev.adapters.plugins.bootstrap import get_plugin_manager
 from Medical_KG_rev.adapters.plugins.manager import AdapterPluginManager
 from Medical_KG_rev.adapters.plugins.models import AdapterRequest
+from Medical_KG_rev.observability.metrics import (
+    record_gate_evaluation,
+    record_gate_timeout,
+    record_phase_transition,
+)
 from Medical_KG_rev.orchestration.dagster.configuration import (
+    GateConditionOperator,
+    GateDefinition,
     PipelineConfigLoader,
     PipelineTopologyConfig,
     StageExecutionHooks,
     ResiliencePolicyLoader,
     StageDefinition,
 )
 from Medical_KG_rev.orchestration.dagster.stages import (
     HaystackPipelineResource,
     build_default_stage_factory,
     create_default_pipeline_resource,
 )
 from Medical_KG_rev.orchestration.events import StageEventEmitter
 from Medical_KG_rev.orchestration.kafka import KafkaClient
-from Medical_KG_rev.orchestration.ledger import JobLedger, JobLedgerError
+from Medical_KG_rev.orchestration.ledger import JobLedger, JobLedgerEntry, JobLedgerError
 from Medical_KG_rev.orchestration.openlineage import OpenLineageEmitter
 from Medical_KG_rev.orchestration.stages.contracts import StageContext
 from Medical_KG_rev.utils.logging import get_logger

 logger = get_logger(__name__)


 class StageResolutionError(RuntimeError):
     """Raised when a stage cannot be resolved from the registry."""


 @dataclass(slots=True)
 class StageFactory:
     """Resolve orchestration stages by topology stage type."""

     registry: Mapping[str, Callable[[StageDefinition], object]]

     def resolve(self, pipeline: str, stage: StageDefinition) -> object:
         try:
             factory = self.registry[stage.stage_type]
         except KeyError as exc:  # pragma: no cover - defensive guard
             raise StageResolutionError(
                 f"Pipeline '{pipeline}' declared unknown stage type '{stage.stage_type}'"
             ) from exc
         instance = factory(stage)
         logger.debug(
             "dagster.stage.resolved",
             pipeline=pipeline,
             stage=stage.name,
             stage_type=stage.stage_type,
         )
         return instance


+class GateConditionError(RuntimeError):
+    """Raised when a gate condition fails or times out."""
+
+    def __init__(
+        self,
+        gate: str,
+        message: str,
+        *,
+        timeout: bool = False,
+        result: GateEvaluationResult | None = None,
+    ) -> None:
+        super().__init__(message)
+        self.gate = gate
+        self.timeout = timeout
+        self.result = result
+
+
+@dataclass(slots=True)
+class GateEvaluationResult:
+    """Outcome of a gate evaluation."""
+
+    gate: str
+    satisfied: bool
+    attempts: int
+    duration_seconds: float
+    observed: Mapping[str, Any]
+    resume_stage: str | None = None
+    last_error: str | None = None
+
+
+class GateConditionEvaluator:
+    """Evaluate gate predicates against ledger entries."""
+
+    def __init__(self, definition: GateDefinition) -> None:
+        self._definition = definition
+
+    def evaluate(
+        self,
+        entry: JobLedgerEntry,
+        *,
+        previous_observed: Mapping[str, Any] | None = None,
+    ) -> tuple[bool, dict[str, Any]]:
+        observed: dict[str, Any] = {}
+        previous_observed = previous_observed or {}
+        satisfied: list[bool] = []
+        for clause in self._definition.condition.clauses:
+            value = self._resolve_field(entry, clause.field)
+            observed[clause.field] = value
+            if clause.operator is GateConditionOperator.EQUALS:
+                satisfied.append(value == clause.value)
+            elif clause.operator is GateConditionOperator.EXISTS:
+                satisfied.append(value is not None)
+            elif clause.operator is GateConditionOperator.CHANGED:
+                satisfied.append(previous_observed.get(clause.field) != value)
+            else:  # pragma: no cover - defensive guard
+                satisfied.append(False)
+        if self._definition.condition.logic == "any":
+            overall = any(satisfied)
+        else:
+            overall = all(satisfied)
+        return overall, observed
+
+    @staticmethod
+    def _resolve_field(entry: JobLedgerEntry, field: str) -> Any:
+        target: Any = entry
+        for part in field.split('.'):
+            if isinstance(target, Mapping):
+                target = target.get(part)
+            else:
+                target = getattr(target, part, None)
+            if target is None:
+                break
+        return target
+
+
+class GateStage:
+    """Runtime executor for gate stages."""
+
+    def __init__(
+        self,
+        gate: GateDefinition,
+        *,
+        ledger: JobLedger,
+        evaluator: GateConditionEvaluator,
+        sleep: Callable[[float], None] = time.sleep,
+    ) -> None:
+        self._gate = gate
+        self._ledger = ledger
+        self._evaluator = evaluator
+        self._sleep = sleep
+
+    def execute(self, ctx: StageContext, state: dict[str, Any]) -> GateEvaluationResult:
+        job_id = ctx.job_id or state.get("job_id")
+        if not job_id:
+            raise GateConditionError(self._gate.name, "Gate stages require a job identifier")
+
+        attempts = 0
+        start = time.perf_counter()
+        overall_deadline = None
+        if self._gate.timeout_seconds:
+            overall_deadline = time.monotonic() + float(self._gate.timeout_seconds)
+        previous_result = (state.get("gate_results", {}) or {}).get(self._gate.name, {})
+        last_error: str | None = None
+        last_observed: dict[str, Any] = dict(previous_result.get("observed") or {})
+        timeout_triggered = False
+
+        retry_config = self._gate.retry
+        max_attempts = retry_config.max_attempts if retry_config else 1
+        backoff = retry_config.backoff_seconds if retry_config else 0.0
+        condition_timeout = self._gate.condition.timeout_seconds
+        poll_interval = self._gate.condition.poll_interval_seconds
+
+        while attempts < max_attempts:
+            attempts += 1
+            attempt_start = time.monotonic()
+            while True:
+                now = time.monotonic()
+                if overall_deadline and now >= overall_deadline:
+                    timeout_triggered = True
+                    last_error = "Gate evaluation exceeded overall timeout"
+                    break
+                entry = self._ledger.get(job_id)
+                if entry is None:
+                    raise GateConditionError(self._gate.name, f"Job {job_id} not found in ledger")
+                satisfied, observed = self._evaluator.evaluate(
+                    entry,
+                    previous_observed=previous_result.get("observed"),
+                )
+                last_observed = dict(observed)
+                previous_result = {"observed": last_observed}
+                logger.debug(
+                    "dagster.gate.evaluate",
+                    gate=self._gate.name,
+                    job_id=job_id,
+                    satisfied=satisfied,
+                    attempts=attempts,
+                )
+                if satisfied:
+                    duration = time.perf_counter() - start
+                    result = GateEvaluationResult(
+                        gate=self._gate.name,
+                        satisfied=True,
+                        attempts=attempts,
+                        duration_seconds=duration,
+                        observed=observed,
+                        resume_stage=self._gate.resume_stage,
+                    )
+                    return result
+                if condition_timeout and now - attempt_start >= condition_timeout:
+                    timeout_triggered = True
+                    last_error = "Gate condition timed out"
+                    break
+                self._sleep(poll_interval)
+            if timeout_triggered or attempts >= max_attempts:
+                break
+            if backoff:
+                self._sleep(backoff)
+
+        duration = time.perf_counter() - start
+        result = GateEvaluationResult(
+            gate=self._gate.name,
+            satisfied=False,
+            attempts=attempts,
+            duration_seconds=duration,
+            observed=last_observed,
+            resume_stage=self._gate.resume_stage,
+            last_error=last_error or "Gate condition not satisfied",
+        )
+        raise GateConditionError(
+            self._gate.name,
+            result.last_error or "Gate failed",
+            timeout=timeout_triggered,
+            result=result,
+        )
+
+
 @op(
     name="bootstrap",
     out=Out(dict),
     config_schema={
         "context": dict,
         "adapter_request": dict,
         "payload": dict,
     },
 )
 def bootstrap_op(context) -> dict[str, Any]:
     """Initialise the orchestration state for a Dagster run."""

     ctx_payload = context.op_config["context"]
     adapter_payload = context.op_config["adapter_request"]
     payload = context.op_config.get("payload", {})

     stage_ctx = StageContext(
         tenant_id=ctx_payload["tenant_id"],
         job_id=ctx_payload.get("job_id"),
         doc_id=ctx_payload.get("doc_id"),
         correlation_id=ctx_payload.get("correlation_id"),
         metadata=ctx_payload.get("metadata", {}),
         pipeline_name=ctx_payload.get("pipeline_name"),
         pipeline_version=ctx_payload.get("pipeline_version"),
     )
@@ -108,132 +291,229 @@ def bootstrap_op(context) -> dict[str, Any]:
         "payload": payload,
         "results": {},
         "job_id": stage_ctx.job_id,
     }
     logger.debug(
         "dagster.bootstrap.initialised",
         tenant_id=stage_ctx.tenant_id,
         pipeline=stage_ctx.pipeline_name,
     )
     return state


 def _stage_state_key(stage_type: str) -> str:
     return {
         "ingest": "payloads",
         "parse": "document",
         "ir-validation": "document",
         "chunk": "chunks",
         "embed": "embedding_batch",
         "index": "index_receipt",
         "extract": "extraction",
         "knowledge-graph": "graph_receipt",
     }.get(stage_type, stage_type)


+def _resolve_gate_definition(
+    topology: PipelineTopologyConfig, stage: StageDefinition
+) -> GateDefinition:
+    gate_name = stage.gate_name or stage.name
+    for definition in topology.gates:
+        if definition.name == gate_name:
+            return definition
+    raise ValueError(f"No gate definition found for stage '{stage.name}'")
+
+
 def _apply_stage_output(
     stage_type: str,
     stage_name: str,
     state: dict[str, Any],
     output: Any,
 ) -> dict[str, Any]:
     if stage_type == "ingest":
         state["payloads"] = output
     elif stage_type in {"parse", "ir-validation"}:
         state["document"] = output
     elif stage_type == "chunk":
         state["chunks"] = output
     elif stage_type == "embed":
         state["embedding_batch"] = output
     elif stage_type == "index":
         state["index_receipt"] = output
     elif stage_type == "extract":
         entities, claims = output
         state["entities"] = entities
         state["claims"] = claims
     elif stage_type == "knowledge-graph":
         state["graph_receipt"] = output
     else:  # pragma: no cover - guard for future expansion
         state[_stage_state_key(stage_type)] = output
     state.setdefault("results", {})[stage_name] = {
         "type": stage_type,
         "output": state.get(_stage_state_key(stage_type)),
     }
     return state


+def _apply_gate_result(
+    topology: PipelineTopologyConfig,
+    stage_definition: StageDefinition,
+    state: dict[str, Any],
+    result: GateEvaluationResult,
+    phase_map: Mapping[str, int],
+) -> dict[str, Any]:
+    updated = dict(state)
+    gate_results = {**(updated.get("gate_results") or {})}
+    gate_results[result.gate] = {
+        "satisfied": result.satisfied,
+        "attempts": result.attempts,
+        "duration_seconds": result.duration_seconds,
+        "resume_stage": result.resume_stage,
+        "resume_phase": phase_map.get(result.resume_stage),
+        "observed": dict(result.observed),
+        "last_error": result.last_error,
+        "recorded_at": time.time(),
+    }
+    updated["gate_results"] = gate_results
+
+    phases = dict(updated.get("phases") or {})
+    map_snapshot = dict(phases.get("map") or {})
+    if not map_snapshot:
+        map_snapshot = dict(phase_map)
+    phases.setdefault("map", map_snapshot)
+    phases.setdefault("history", [])
+    phases.setdefault("completed", [])
+    phases.setdefault("transitions", [])
+    current_phase = phase_map.get(stage_definition.name, phases.get("current", 1))
+    phases["current"] = current_phase
+    pending = list(phases.get("pending", []))
+    if result.satisfied:
+        if result.gate in pending:
+            pending = [gate for gate in pending if gate != result.gate]
+        if result.gate not in phases["completed"]:
+            phases["completed"].append(result.gate)
+        phases["history"].append(
+            {
+                "gate": result.gate,
+                "phase": current_phase,
+                "timestamp": time.time(),
+            }
+        )
+        next_phase = phase_map.get(result.resume_stage, current_phase + 1)
+        phases["current"] = next_phase
+        label = f"phase-{next_phase}"
+        if label not in phases["transitions"]:
+            phases["transitions"].append(label)
+            record_phase_transition(topology.name, label)
+    else:
+        if result.gate not in pending:
+            pending.append(result.gate)
+    phases["pending"] = pending
+    updated["phases"] = phases
+    updated.setdefault("results", {})[stage_definition.name] = {
+        "type": "gate",
+        "output": None,
+        "satisfied": result.satisfied,
+    }
+    return updated
+
+
 def _infer_output_count(stage_type: str, output: Any) -> int:
     if output is None:
         return 0
     if stage_type in {"ingest", "chunk"} and isinstance(output, Sequence):
         return len(output)
     if stage_type in {"parse", "ir-validation"}:
         return 1
     if stage_type == "embed" and hasattr(output, "vectors"):
         vectors = getattr(output, "vectors")
         if isinstance(vectors, Sequence):
             return len(vectors)
     if stage_type == "index" and hasattr(output, "chunks_indexed"):
         indexed = getattr(output, "chunks_indexed")
         if isinstance(indexed, int):
             return indexed
     if stage_type == "extract" and isinstance(output, tuple) and len(output) == 2:
         entities, claims = output
         entity_count = len(entities) if isinstance(entities, Sequence) else 0
         claim_count = len(claims) if isinstance(claims, Sequence) else 0
         return entity_count + claim_count
     if stage_type == "knowledge-graph" and hasattr(output, "nodes_written"):
         nodes = getattr(output, "nodes_written", 0)
         if isinstance(nodes, int):
             return nodes
     return 1


 def _make_stage_op(
     topology: PipelineTopologyConfig,
     stage_definition: StageDefinition,
+    phase_map: Mapping[str, int],
 ):
     stage_type = stage_definition.stage_type
     stage_name = stage_definition.name
     policy_name = stage_definition.policy or "default"
+    stage_phase = phase_map.get(stage_name, 1)

     @op(
         name=stage_name,
         ins={"state": In(dict)},
         out=Out(dict),
         required_resource_keys={
             "stage_factory",
             "resilience_policies",
             "job_ledger",
             "event_emitter",
         },
     )
     def _stage_op(context, state: dict[str, Any]) -> dict[str, Any]:
-        stage = context.resources.stage_factory.resolve(topology.name, stage_definition)
         policy_loader: ResiliencePolicyLoader = context.resources.resilience_policies
-
-        execute = getattr(stage, "execute")
+        stage_factory: StageFactory = context.resources.stage_factory
+        ledger: JobLedger = context.resources.job_ledger
+        emitter: StageEventEmitter = context.resources.event_emitter
+
+        phases_state = state.get("phases", {}) or {}
+        current_phase = phases_state.get("current", 1)
+        if stage_phase < current_phase:
+            logger.debug(
+                "dagster.stage.skipped",
+                pipeline=topology.name,
+                stage=stage_name,
+                reason="phase completed",
+                phase=stage_phase,
+                current_phase=current_phase,
+            )
+            return state
+
+        if stage_type == "gate":
+            gate_definition = _resolve_gate_definition(topology, stage_definition)
+            evaluator = GateConditionEvaluator(gate_definition)
+            gate_stage = GateStage(gate_definition, ledger=ledger, evaluator=evaluator)
+            execute = gate_stage.execute
+        else:
+            stage = stage_factory.resolve(topology.name, stage_definition)
+            execute = getattr(stage, "execute")
         execution_state: dict[str, Any] = {
             "attempts": 0,
             "duration": 0.0,
             "failed": False,
             "error": None,
         }

         def _on_retry(retry_state: Any) -> None:
             job_identifier = state.get("job_id")
             if job_identifier:
                 ledger.increment_retry(job_identifier, stage_name)
             sleep_seconds = getattr(getattr(retry_state, "next_action", None), "sleep", 0.0) or 0.0
             attempt_number = getattr(retry_state, "attempt_number", 0) + 1
             error = getattr(getattr(retry_state, "outcome", None), "exception", lambda: None)()
             reason = str(error) if error else "retry"
             emitter.emit_retrying(
                 state["context"],
                 stage_name,
                 attempt=attempt_number,
                 backoff_ms=int(sleep_seconds * 1000),
                 reason=reason,
             )

         def _on_success(attempts: int, duration: float) -> None:
             execution_state["attempts"] = attempts
@@ -242,99 +522,175 @@ def _make_stage_op(
         def _on_failure(error: BaseException, attempts: int) -> None:
             execution_state["attempts"] = attempts
             execution_state["failed"] = True
             execution_state["error"] = error

         hooks = StageExecutionHooks(
             on_retry=_on_retry,
             on_success=_on_success,
             on_failure=_on_failure,
         )

         wrapped = policy_loader.apply(policy_name, stage_name, execute, hooks=hooks)

         stage_ctx: StageContext = state["context"]
         job_id = stage_ctx.job_id or state.get("job_id")

         initial_attempt = 1
         if job_id:
             entry = ledger.mark_stage_started(job_id, stage_name)
             initial_attempt = entry.retry_count_per_stage.get(stage_name, 0) + 1
         emitter.emit_started(stage_ctx, stage_name, attempt=initial_attempt)

         start_time = time.perf_counter()

         try:
-            if stage_type == "ingest":
+            if stage_type == "gate":
+                result = wrapped(stage_ctx, state)
+            elif stage_type == "ingest":
                 adapter_request: AdapterRequest = state["adapter_request"]
                 result = wrapped(stage_ctx, adapter_request)
             elif stage_type in {"parse", "ir-validation"}:
                 payloads = state.get("payloads", [])
                 result = wrapped(stage_ctx, payloads)
             elif stage_type == "chunk":
                 document = state.get("document")
                 result = wrapped(stage_ctx, document)
             elif stage_type == "embed":
                 chunks = state.get("chunks", [])
                 result = wrapped(stage_ctx, chunks)
             elif stage_type == "index":
                 batch = state.get("embedding_batch")
                 result = wrapped(stage_ctx, batch)
             elif stage_type == "extract":
                 document = state.get("document")
                 result = wrapped(stage_ctx, document)
             elif stage_type == "knowledge-graph":
                 entities = state.get("entities", [])
                 claims = state.get("claims", [])
                 result = wrapped(stage_ctx, entities, claims)
             else:  # pragma: no cover - guard for future expansion
                 upstream = state.get(_stage_state_key(stage_type))
                 result = wrapped(stage_ctx, upstream)
+        except GateConditionError as exc:
+            attempts = execution_state.get("attempts") or 1
+            emitter.emit_failed(stage_ctx, stage_name, attempt=attempts, error=str(exc))
+            if job_id:
+                metadata: dict[str, Any] = {
+                    f"stage.{stage_name}.gate.status": "timeout"
+                    if exc.timeout
+                    else "waiting",
+                    f"stage.{stage_name}.gate.error": str(exc),
+                }
+                if exc.result:
+                    metadata[f"stage.{stage_name}.gate.attempts"] = exc.result.attempts
+                    metadata[f"gate.{exc.result.gate}.status"] = (
+                        "timeout" if exc.timeout else "waiting"
+                    )
+                    metadata[f"gate.{exc.result.gate}.resume_stage"] = exc.result.resume_stage
+                    metadata[f"gate.{exc.result.gate}.attempts"] = exc.result.attempts
+                    metadata[f"gate.{exc.result.gate}.resume_phase"] = phase_map.get(
+                        exc.result.resume_stage, stage_phase + 1
+                    )
+                    if exc.result.last_error:
+                        metadata[f"gate.{exc.result.gate}.error"] = exc.result.last_error
+                ledger.update_metadata(job_id, metadata)
+            if exc.timeout:
+                record_gate_timeout(exc.gate)
+            if exc.result:
+                record_gate_evaluation(
+                    exc.result.gate,
+                    "timeout" if exc.timeout else "failure",
+                    exc.result.duration_seconds,
+                    attempts=exc.result.attempts,
+                )
+            else:
+                record_gate_evaluation(exc.gate, "failure", 0.0, attempts=attempts)
+            raise
         except Exception as exc:
             attempts = execution_state.get("attempts") or 1
             emitter.emit_failed(stage_ctx, stage_name, attempt=attempts, error=str(exc))
             if job_id:
                 ledger.mark_failed(job_id, stage=stage_name, reason=str(exc))
             raise

         updated = dict(state)
-        _apply_stage_output(stage_type, stage_name, updated, result)
-        output = updated.get(_stage_state_key(stage_type))
+        gate_result: GateEvaluationResult | None = None
+        if stage_type == "gate":
+            gate_result = result
+            updated = _apply_gate_result(
+                topology,
+                stage_definition,
+                updated,
+                gate_result,
+                phase_map,
+            )
+            output = None
+        else:
+            _apply_stage_output(stage_type, stage_name, updated, result)
+            output = updated.get(_stage_state_key(stage_type))
         attempts = execution_state.get("attempts") or 1
         duration_seconds = execution_state.get("duration") or (time.perf_counter() - start_time)
         duration_ms = int(duration_seconds * 1000)
-        output_count = _infer_output_count(stage_type, output)
+        output_count = 0 if stage_type == "gate" else _infer_output_count(stage_type, output)

         if job_id:
             ledger.update_metadata(
                 job_id,
                 {
                     f"stage.{stage_name}.attempts": attempts,
                     f"stage.{stage_name}.output_count": output_count,
                     f"stage.{stage_name}.duration_ms": duration_ms,
+                    **(
+                        {
+                            f"stage.{stage_name}.gate.status": "passed"
+                            if gate_result and gate_result.satisfied
+                            else "waiting",
+                            f"stage.{stage_name}.gate.resume": gate_result.resume_stage
+                            if gate_result
+                            else None,
+                            f"gate.{gate_result.gate}.status": "passed"
+                            if gate_result and gate_result.satisfied
+                            else "waiting",
+                            f"gate.{gate_result.gate}.resume_stage": gate_result.resume_stage,
+                            f"gate.{gate_result.gate}.attempts": gate_result.attempts,
+                            f"gate.{gate_result.gate}.resume_phase": phase_map.get(
+                                gate_result.resume_stage, stage_phase + 1
+                            ),
+                        }
+                        if gate_result
+                        else {}
+                    ),
                 },
             )
+        if gate_result:
+            record_gate_evaluation(
+                gate_result.gate,
+                "success" if gate_result.satisfied else "failure",
+                gate_result.duration_seconds,
+                attempts=gate_result.attempts,
+            )
         emitter.emit_completed(
             stage_ctx,
             stage_name,
             attempt=attempts,
             duration_ms=duration_ms,
             output_count=output_count,
         )
         logger.debug(
             "dagster.stage.completed",
             pipeline=topology.name,
             stage=stage_name,
             stage_type=stage_type,
             policy=policy_name,
             attempts=attempts,
             duration_ms=duration_ms,
             output_count=output_count,
         )
         return updated

     return _stage_op


 def _topological_order(stages: list[StageDefinition]) -> list[str]:
     graph: dict[str, set[str]] = {stage.name: set(stage.depends_on) for stage in stages}
     resolved: list[str] = []
@@ -342,100 +698,168 @@ def _topological_order(stages: list[StageDefinition]) -> list[str]:
     permanent: set[str] = set()

     def visit(node: str) -> None:
         if node in permanent:
             return
         if node in temporary:
             raise ValueError(f"Cycle detected involving stage '{node}'")
         temporary.add(node)
         for dep in graph.get(node, set()):
             visit(dep)
         temporary.remove(node)
         permanent.add(node)
         resolved.append(node)

     for stage in graph:
         visit(stage)
     return resolved


 @dataclass(slots=True)
 class BuiltPipelineJob:
     job_name: str
     job_definition: Any
     final_node: str
     version: str
+    topology: PipelineTopologyConfig
+    phase_map: Mapping[str, int]


 def _normalise_name(name: str) -> str:
     """Return a Dagster-safe identifier derived from the pipeline name."""

     candidate = re.sub(r"[^0-9A-Za-z_]+", "_", name)
     if not candidate:
         return "pipeline"
     if candidate[0].isdigit():
         candidate = f"p_{candidate}"
     return candidate


 def _build_pipeline_job(
     topology: PipelineTopologyConfig,
     *,
     resource_defs: Mapping[str, ResourceDefinition],
 ) -> BuiltPipelineJob:
+    order = _topological_order(topology.stages)
+    stage_lookup = {stage.name: stage for stage in topology.stages}
+    phase_map: dict[str, int] = {}
+    phase_index = 1
+    for stage_name in order:
+        definition = stage_lookup[stage_name]
+        phase_map[stage_name] = phase_index
+        if definition.stage_type == "gate":
+            phase_index += 1
+    for stage_name in order:
+        stage_phase = phase_map[stage_name]
+        for dependency in stage_lookup[stage_name].depends_on:
+            dep_phase = phase_map.get(dependency, stage_phase)
+            if dep_phase > stage_phase:
+                raise ValueError(
+                    f"stage '{stage_name}' depends on future stage '{dependency}' across gate boundary"
+                )
     stage_ops = {
-        stage.name: _make_stage_op(topology, stage)
+        stage.name: _make_stage_op(topology, stage, phase_map)
         for stage in topology.stages
     }
-    order = _topological_order(topology.stages)
+    phase_groups: dict[int, list[str]] = {}
+    for stage_name in order:
+        phase_groups.setdefault(phase_map[stage_name], []).append(stage_name)

     safe_name = _normalise_name(topology.name)

+    @op(name="initialise_state", ins={"state": In(dict)}, out=Out(dict))
+    def _initialise_state_op(state: dict[str, Any]) -> dict[str, Any]:
+        updated = dict(state)
+        phases = dict(updated.get("phases") or {})
+        phases.setdefault("current", 1)
+        phases.setdefault("map", dict(phase_map))
+        phases.setdefault("completed", [])
+        phases.setdefault("history", [])
+        phases.setdefault("transitions", [])
+        payload = updated.get("payload") or {}
+        resume_phase = payload.get("resume_phase")
+        resume_stage = payload.get("resume_stage")
+        if resume_phase is None and resume_stage:
+            resume_phase = phase_map.get(str(resume_stage))
+        if isinstance(resume_phase, str) and resume_phase.isdigit():
+            resume_phase = int(resume_phase)
+        if isinstance(resume_phase, int) and resume_phase >= 1:
+            phases["current"] = resume_phase
+        updated["phases"] = phases
+        gate_results = dict(updated.get("gate_results") or {})
+        gate_results_payload = payload.get("gate_results") or {}
+        if isinstance(gate_results_payload, Mapping):
+            gate_results.update(gate_results_payload)  # type: ignore[arg-type]
+        updated["gate_results"] = gate_results
+        return updated
+
+    def _make_phase_graph(phase: int, stages_for_phase: list[str]):
+        graph_name = f"{safe_name}_phase_{phase}"
+
+        @graph(name=graph_name)
+        def _phase_graph(state):
+            current = state
+            for stage_name in stages_for_phase:
+                op_def = stage_ops[stage_name].alias(stage_name)
+                current = op_def(current)
+            return current
+
+        return _phase_graph
+
+    phase_graphs = {
+        phase: _make_phase_graph(phase, stage_names)
+        for phase, stage_names in phase_groups.items()
+    }
+
     @graph(name=f"{safe_name}_graph")
     def _pipeline_graph():
         state = bootstrap_op.alias("bootstrap")()
-        for stage_name in order:
-            op_def = stage_ops[stage_name].alias(stage_name)
-            state = op_def(state)
+        state = _initialise_state_op.alias("initialise_state")(state)
+        for phase in sorted(phase_graphs):
+            phase_node = phase_graphs[phase].alias(f"phase_{phase}")
+            state = phase_node(state)
         return state

     job = _pipeline_graph.to_job(
         name=f"{safe_name}_job",
         resource_defs={
             **resource_defs,
         },
         tags={
             "medical_kg.pipeline": topology.name,
             "medical_kg.pipeline_version": topology.version,
         },
     )

     return BuiltPipelineJob(
         job_name=job.name,
         job_definition=job,
         final_node=order[-1] if order else "bootstrap",
         version=topology.version,
+        topology=topology,
+        phase_map=phase_map,
     )


 @dataclass(slots=True)
 class DagsterRunResult:
     """Result returned after executing a Dagster job."""

     pipeline: str
     success: bool
     state: dict[str, Any]
     dagster_result: ExecuteInProcessResult


 class DagsterOrchestrator:
     """Submit orchestration jobs to Dagster using declarative topology configs."""

     def __init__(
         self,
         pipeline_loader: PipelineConfigLoader,
         resilience_loader: ResiliencePolicyLoader,
         stage_factory: StageFactory,
         *,
         plugin_manager: AdapterPluginManager | None = None,
         job_ledger: JobLedger | None = None,
         kafka_client: KafkaClient | None = None,
@@ -618,85 +1042,119 @@ class DagsterOrchestrator:
 def submit_to_dagster(
     orchestrator: DagsterOrchestrator,
     *,
     pipeline: str,
     context: StageContext,
     adapter_request: AdapterRequest,
     payload: Mapping[str, Any] | None = None,
 ) -> DagsterRunResult:
     """Convenience helper mirroring the legacy orchestration API."""

     return orchestrator.submit(
         pipeline=pipeline,
         context=context,
         adapter_request=adapter_request,
         payload=payload or {},
     )


 @sensor(name="pdf_ir_ready_sensor", minimum_interval_seconds=30, required_resource_keys={"job_ledger"})
 def pdf_ir_ready_sensor(context: SensorEvaluationContext):
     ledger: JobLedger = context.resources.job_ledger
     ready_requests: list[RunRequest] = []
     for entry in ledger.all():
         if entry.pipeline_name != "pdf-two-phase":
             continue
-        if not entry.pdf_ir_ready or entry.status != "processing":
+        gate_status = entry.metadata.get("gate.pdf_ir_ready.status")
+        resume_stage = entry.metadata.get("gate.pdf_ir_ready.resume_stage")
+        resume_phase = entry.metadata.get("gate.pdf_ir_ready.resume_phase")
+        resume_requested = entry.metadata.get("gate.pdf_ir_ready.resume_requested")
+        if not entry.pdf_ir_ready or entry.status not in {"processing", "queued"}:
+            continue
+        if gate_status != "passed" or not resume_stage or resume_requested:
             continue
+        try:
+            parsed_phase = int(resume_phase) if resume_phase is not None else None
+        except (TypeError, ValueError):  # pragma: no cover - defensive guard
+            parsed_phase = None
+
         run_key = f"{entry.job_id}-resume"
         context_payload = {
             "tenant_id": entry.tenant_id,
             "job_id": entry.job_id,
             "doc_id": entry.doc_key,
             "correlation_id": entry.metadata.get("correlation_id"),
             "metadata": dict(entry.metadata),
             "pipeline_name": entry.pipeline_name,
-            "pipeline_version": entry.metadata.get("pipeline_version", entry.pipeline_name or ""),
+            "pipeline_version": entry.metadata.get(
+                "pipeline_version", entry.pipeline_name or ""
+            ),
         }
         adapter_payload = entry.metadata.get("adapter_request", {})
-        payload = entry.metadata.get("payload", {})
+        payload = dict(entry.metadata.get("payload", {}) or {})
+        gate_results = dict(payload.get("gate_results") or {})
+        gate_results.setdefault(
+            "pdf_ir_ready",
+            {
+                "satisfied": True,
+                "resume_stage": resume_stage,
+                "resume_phase": parsed_phase,
+            },
+        )
+        payload.update(
+            {
+                "resume_stage": resume_stage,
+                "resume_phase": parsed_phase,
+                "gate_results": gate_results,
+            }
+        )
         run_config = {
             "ops": {
                 "bootstrap": {
                     "config": {
                         "context": context_payload,
                         "adapter_request": adapter_payload,
                         "payload": payload,
                     }
                 }
             }
         }
         ready_requests.append(
             RunRequest(
                 run_key=run_key,
                 run_config=run_config,
                 tags={
                     "medical_kg.pipeline": entry.pipeline_name or "",
-                    "medical_kg.resume_stage": "chunk",
+                    "medical_kg.resume_stage": resume_stage,
+                    "medical_kg.resume_phase": str(parsed_phase) if parsed_phase else "",
                 },
             )
         )
+        ledger.update_metadata(
+            entry.job_id,
+            {f"gate.pdf_ir_ready.resume_requested": True},
+        )
     if not ready_requests:
         yield SkipReason("No PDF ingestion jobs ready for resumption")
         return
     for request in ready_requests:
         yield request


 def build_default_orchestrator() -> DagsterOrchestrator:
     """Construct a Dagster orchestrator with default stage builders."""

     pipeline_loader = PipelineConfigLoader()
     resilience_loader = ResiliencePolicyLoader()
     plugin_manager = get_plugin_manager()
     pipeline_resource = create_default_pipeline_resource()
     stage_builders = build_default_stage_factory(plugin_manager, pipeline_resource)
     stage_factory = StageFactory(stage_builders)
     job_ledger = JobLedger()
     kafka_client = KafkaClient()
     event_emitter = StageEventEmitter(kafka_client)
     openlineage_emitter = OpenLineageEmitter()
     return DagsterOrchestrator(
         pipeline_loader,
         resilience_loader,
         stage_factory,
         plugin_manager=plugin_manager,
diff --git a/tests/orchestration/test_dagster_gates.py b/tests/orchestration/test_dagster_gates.py
new file mode 100644
index 0000000000000000000000000000000000000000..36adad85ec3469fe489f6565a0c2c5e6d4e35935
--- /dev/null
+++ b/tests/orchestration/test_dagster_gates.py
@@ -0,0 +1,223 @@
+from __future__ import annotations
+
+import json
+from typing import Any
+
+import pytest
+from dagster import ResourceDefinition
+
+from Medical_KG_rev.orchestration.dagster.configuration import (
+    GateCondition,
+    GateConditionClause,
+    GateConditionOperator,
+    GateDefinition,
+    GateRetryPolicy,
+    PipelineTopologyConfig,
+)
+from Medical_KG_rev.orchestration.dagster.runtime import (
+    GateConditionError,
+    GateConditionEvaluator,
+    GateStage,
+    StageFactory,
+    _build_pipeline_job,
+)
+from Medical_KG_rev.orchestration.ledger import JobLedger, JobLedgerEntry
+from Medical_KG_rev.orchestration.stages.contracts import StageContext
+
+
+class _StubStage:
+    def __init__(self, result: Any = None) -> None:
+        self._result = result
+
+    def execute(self, ctx: StageContext, *_: Any, **__: Any) -> Any:
+        return self._result
+
+
+class _StubPolicies:
+    def apply(self, _name: str, _stage: str, func, *, hooks=None):  # type: ignore[override]
+        return func
+
+
+class _StubEmitter:
+    def emit_retrying(self, *args, **kwargs):  # pragma: no cover - instrumentation stub
+        return None
+
+    def emit_started(self, *args, **kwargs):  # pragma: no cover
+        return None
+
+    def emit_failed(self, *args, **kwargs):  # pragma: no cover
+        return None
+
+    def emit_completed(self, *args, **kwargs):  # pragma: no cover
+        return None
+
+
+@pytest.fixture()
+def gate_definition() -> GateDefinition:
+    return GateDefinition(
+        name="pdf_ir_ready",
+        resume_stage="chunk",
+        timeout_seconds=30,
+        retry=GateRetryPolicy(max_attempts=2, backoff_seconds=0.0),
+        condition=GateCondition(
+            logic="all",
+            poll_interval_seconds=0.5,
+            timeout_seconds=5,
+            clauses=[
+                GateConditionClause(
+                    field="pdf_ir_ready",
+                    operator=GateConditionOperator.EQUALS,
+                    value=True,
+                ),
+                GateConditionClause(
+                    field="metadata.gate.pdf_ir_ready.status",
+                    operator=GateConditionOperator.EQUALS,
+                    value="passed",
+                ),
+            ],
+        ),
+    )
+
+
+def test_gate_condition_evaluator_supports_changed_operator(gate_definition: GateDefinition) -> None:
+    changed_definition = gate_definition.model_copy(
+        update={
+            "condition": GateCondition(
+                logic="all",
+                poll_interval_seconds=0.5,
+                timeout_seconds=5,
+                clauses=[
+                    GateConditionClause(
+                        field="pdf_ir_ready",
+                        operator=GateConditionOperator.EQUALS,
+                        value=True,
+                    ),
+                    GateConditionClause(
+                        field="metadata.gate.pdf_ir_ready.status",
+                        operator=GateConditionOperator.CHANGED,
+                    ),
+                ],
+            )
+        }
+    )
+    evaluator = GateConditionEvaluator(changed_definition)
+    entry = JobLedgerEntry(job_id="job-1", doc_key="doc-1", tenant_id="tenant")
+    entry.metadata["gate"] = {"pdf_ir_ready": {"status": "passed"}}
+    entry.pdf_ir_ready = True
+
+    satisfied, observed = evaluator.evaluate(
+        entry,
+        previous_observed={"pdf_ir_ready": False, "metadata.gate.pdf_ir_ready.status": "waiting"},
+    )
+
+    assert satisfied is True
+    assert observed["pdf_ir_ready"] is True
+    assert observed["metadata.gate.pdf_ir_ready.status"] == "passed"
+
+
+def test_gate_stage_returns_success_when_conditions_met(gate_definition: GateDefinition) -> None:
+    ledger = JobLedger()
+    entry = ledger.create(job_id="job-1", doc_key="doc-1", tenant_id="tenant")
+    ledger.set_pdf_ir_ready(entry.job_id, True)
+    ledger.update_metadata(
+        entry.job_id,
+        {
+            "gate": {"pdf_ir_ready": {"status": "passed"}},
+            "gate.pdf_ir_ready.status": "passed",
+        },
+    )
+
+    stage = GateStage(
+        gate_definition,
+        ledger=ledger,
+        evaluator=GateConditionEvaluator(gate_definition),
+        sleep=lambda seconds: None,
+    )
+    context = StageContext(tenant_id="tenant", job_id=entry.job_id)
+    state: dict[str, Any] = {"job_id": entry.job_id}
+
+    result = stage.execute(context, state)
+
+    assert result.satisfied is True
+    assert result.resume_stage == "chunk"
+    assert result.observed["pdf_ir_ready"] is True
+
+
+def test_gate_stage_times_out(monkeypatch: pytest.MonkeyPatch, gate_definition: GateDefinition) -> None:
+    ledger = JobLedger()
+    entry = ledger.create(job_id="job-2", doc_key="doc-2", tenant_id="tenant")
+    clock = {"now": 0.0}
+
+    def advance(seconds: float) -> None:
+        clock["now"] += seconds
+
+    monkeypatch.setattr(
+        "Medical_KG_rev.orchestration.dagster.runtime.time.monotonic",
+        lambda: clock["now"],
+    )
+    monkeypatch.setattr(
+        "Medical_KG_rev.orchestration.dagster.runtime.time.perf_counter",
+        lambda: clock["now"],
+    )
+
+    timeout_definition = gate_definition.model_copy(
+        update={
+            "timeout_seconds": 1,
+            "condition": gate_definition.condition.model_copy(
+                update={"timeout_seconds": 1, "poll_interval_seconds": 0.5}
+            ),
+        }
+    )
+    stage = GateStage(
+        timeout_definition,
+        ledger=ledger,
+        evaluator=GateConditionEvaluator(timeout_definition),
+        sleep=advance,
+    )
+    context = StageContext(tenant_id="tenant", job_id=entry.job_id)
+
+    with pytest.raises(GateConditionError) as excinfo:
+        stage.execute(context, {"job_id": entry.job_id})
+
+    error = excinfo.value
+    assert isinstance(error, GateConditionError)
+    assert error.timeout is True
+    assert error.gate == "pdf_ir_ready"
+
+
+def test_build_pipeline_job_records_phase_map(gate_definition: GateDefinition) -> None:
+    topology_payload = {
+        "name": "gated-pipeline",
+        "version": "2025-01-01",
+        "stages": [
+            {"name": "ingest", "type": "ingest"},
+            {
+                "name": "gate_pdf_ir_ready",
+                "type": "gate",
+                "depends_on": ["ingest"],
+                "gate": "pdf_ir_ready",
+            },
+            {"name": "chunk", "type": "chunk", "depends_on": ["gate_pdf_ir_ready"]},
+        ],
+        "gates": [json.loads(gate_definition.model_dump_json())],
+    }
+    topology = PipelineTopologyConfig.model_validate(topology_payload)
+
+    stage_factory = StageFactory(
+        {
+            "ingest": lambda _: _StubStage([{"payload": "ok"}]),
+            "chunk": lambda _: _StubStage([]),
+        }
+    )
+    resource_defs = {
+        "stage_factory": ResourceDefinition.hardcoded_resource(stage_factory),
+        "resilience_policies": ResourceDefinition.hardcoded_resource(_StubPolicies()),
+        "job_ledger": ResourceDefinition.hardcoded_resource(JobLedger()),
+        "event_emitter": ResourceDefinition.hardcoded_resource(_StubEmitter()),
+    }
+
+    built = _build_pipeline_job(topology, resource_defs=resource_defs)
+
+    assert built.phase_map["ingest"] == 1
+    assert built.phase_map["gate_pdf_ir_ready"] == 1
+    assert built.phase_map["chunk"] == 2
diff --git a/tests/orchestration/test_dagster_sensors.py b/tests/orchestration/test_dagster_sensors.py
index 045b0ab34fdba8c009668b178c67d178a8427198..9a6315d862d0790ce08d61c1d8ec3c91fcd8bb12 100644
--- a/tests/orchestration/test_dagster_sensors.py
+++ b/tests/orchestration/test_dagster_sensors.py
@@ -16,36 +16,55 @@ def test_pdf_ir_sensor_skips_when_no_jobs() -> None:
     assert isinstance(results[0], SkipReason)


 def test_pdf_ir_sensor_emits_run_request() -> None:
     ledger = JobLedger()
     job_id = "job-sensor-1"
     ledger.create(
         job_id=job_id,
         doc_key="doc-sensor",
         tenant_id="tenant-1",
         pipeline="pdf-two-phase",
         metadata={
             "pipeline_version": "2025-01-01",
             "correlation_id": "corr-sensor",
             "adapter_request": {
                 "tenant_id": "tenant-1",
                 "correlation_id": "corr-sensor",
                 "domain": "biomedical",
                 "parameters": {"dataset": "pmc"},
             },
             "payload": {"dataset": "pmc", "item": {"id": "doc-sensor"}},
         },
     )
     ledger.mark_processing(job_id, stage="gate_pdf_ir_ready")
     ledger.set_pdf_ir_ready(job_id)
+    ledger.update_metadata(
+        job_id,
+        {
+            "gate.pdf_ir_ready.status": "passed",
+            "gate.pdf_ir_ready.resume_stage": "chunk",
+            "gate.pdf_ir_ready.resume_phase": 2,
+            "gate_results": {
+                "pdf_ir_ready": {
+                    "satisfied": True,
+                    "resume_stage": "chunk",
+                    "resume_phase": 2,
+                }
+            },
+        },
+    )

     context = build_sensor_context(resources={"job_ledger": ledger})
     results = list(pdf_ir_ready_sensor(context))

     assert results
     assert all(isinstance(item, RunRequest) for item in results)
     request = results[0]
     assert request.run_key == "job-sensor-1-resume"
     ctx_config = request.run_config["ops"]["bootstrap"]["config"]["context"]
     assert ctx_config["job_id"] == job_id
     assert ctx_config["pipeline_name"] == "pdf-two-phase"
+    payload = request.run_config["ops"]["bootstrap"]["config"]["payload"]
+    assert payload["resume_stage"] == "chunk"
+    assert payload["resume_phase"] == 2
+    assert payload["gate_results"]["pdf_ir_ready"]["satisfied"] is True

EOF
)
