# syntax=docker/dockerfile:1.5
ARG VLLM_BASE_IMAGE=vllm/vllm-openai:v0.11.0
FROM ${VLLM_BASE_IMAGE}

ARG MODEL_SOURCE_DIR=qwen3-embedding-8b
ARG MODEL_NAME=qwen3-embedding-8b
ARG MODEL_ID="Qwen/Qwen2.5-Coder-1.5B"
ARG GPU_MEMORY_UTILIZATION=0.9
ARG MAX_MODEL_LEN=8192

ENV MODEL_ID=${MODEL_ID} \
    MODEL_NAME=${MODEL_NAME} \
    MODEL_PATH=/models/${MODEL_NAME} \
    GPU_MEMORY_UTILIZATION=${GPU_MEMORY_UTILIZATION} \
    MAX_MODEL_LEN=${MAX_MODEL_LEN}

# The model artifacts are expected to be available in the build context under
# models/${MODEL_SOURCE_DIR}. They are mounted read-only inside the container so
# that runtime environments do not mutate shared checkpoints.
COPY models/${MODEL_SOURCE_DIR} ${MODEL_PATH}
COPY config/embedding/vllm.yaml /config/vllm.yaml

CMD [
  "python",
  "-m",
  "vllm.entrypoints.openai.api_server",
  "--config",
  "/config/vllm.yaml"
]
