diff a/src/Medical_KG_rev/orchestration/dagster/runtime.py b/src/Medical_KG_rev/orchestration/dagster/runtime.py	(rejected hunks)
@@ -11,316 +11,455 @@ from uuid import uuid4

 from dagster import (
     Definitions,
     ExecuteInProcessResult,
     In,
     Out,
     ResourceDefinition,
     RunRequest,
     SensorEvaluationContext,
     SkipReason,
     graph,
     op,
     sensor,
 )

 from Medical_KG_rev.adapters.plugins.bootstrap import get_plugin_manager
 from Medical_KG_rev.adapters.plugins.manager import AdapterPluginManager
 from Medical_KG_rev.adapters.plugins.models import AdapterRequest
 from Medical_KG_rev.orchestration.dagster.configuration import (
     PipelineConfigLoader,
     PipelineTopologyConfig,
     StageExecutionHooks,
     ResiliencePolicyLoader,
     StageDefinition,
 )
+from Medical_KG_rev.orchestration.dagster.gates import GateConditionError
 from Medical_KG_rev.orchestration.dagster.stages import (
     HaystackPipelineResource,
     build_default_stage_factory,
     create_default_pipeline_resource,
 )
 from Medical_KG_rev.orchestration.events import StageEventEmitter
 from Medical_KG_rev.orchestration.kafka import KafkaClient
 from Medical_KG_rev.orchestration.ledger import JobLedger, JobLedgerError
 from Medical_KG_rev.orchestration.openlineage import OpenLineageEmitter
 from Medical_KG_rev.orchestration.stages.contracts import StageContext
+from Medical_KG_rev.observability.metrics import (
+    record_gate_evaluation,
+    record_phase_transition,
+)
 from Medical_KG_rev.utils.logging import get_logger

 logger = get_logger(__name__)


 class StageResolutionError(RuntimeError):
     """Raised when a stage cannot be resolved from the registry."""


 @dataclass(slots=True)
 class StageFactory:
     """Resolve orchestration stages by topology stage type."""

-    registry: Mapping[str, Callable[[StageDefinition], object]]
+    registry: Mapping[str, Callable[[PipelineTopologyConfig, StageDefinition], object]]

-    def resolve(self, pipeline: str, stage: StageDefinition) -> object:
+    def resolve(self, topology: PipelineTopologyConfig, stage: StageDefinition) -> object:
         try:
             factory = self.registry[stage.stage_type]
         except KeyError as exc:  # pragma: no cover - defensive guard
             raise StageResolutionError(
-                f"Pipeline '{pipeline}' declared unknown stage type '{stage.stage_type}'"
+                f"Pipeline '{topology.name}' declared unknown stage type '{stage.stage_type}'"
             ) from exc
-        instance = factory(stage)
+        instance = factory(topology, stage)
         logger.debug(
             "dagster.stage.resolved",
-            pipeline=pipeline,
+            pipeline=topology.name,
             stage=stage.name,
             stage_type=stage.stage_type,
         )
         return instance


 @op(
     name="bootstrap",
     out=Out(dict),
     config_schema={
         "context": dict,
         "adapter_request": dict,
         "payload": dict,
     },
 )
 def bootstrap_op(context) -> dict[str, Any]:
     """Initialise the orchestration state for a Dagster run."""

     ctx_payload = context.op_config["context"]
     adapter_payload = context.op_config["adapter_request"]
     payload = context.op_config.get("payload", {})

     stage_ctx = StageContext(
         tenant_id=ctx_payload["tenant_id"],
         job_id=ctx_payload.get("job_id"),
         doc_id=ctx_payload.get("doc_id"),
         correlation_id=ctx_payload.get("correlation_id"),
         metadata=ctx_payload.get("metadata", {}),
         pipeline_name=ctx_payload.get("pipeline_name"),
         pipeline_version=ctx_payload.get("pipeline_version"),
+        phase=ctx_payload.get("phase"),
     )
     adapter_request = AdapterRequest.model_validate(adapter_payload)

+    phase_label = str(ctx_payload.get("phase") or "phase-1")
+    try:
+        phase_index = int(phase_label.split("-", maxsplit=1)[1])
+    except Exception:
+        phase_index = 1
+    phase_ready = bool(ctx_payload.get("phase_ready", phase_index > 1))
+
     state = {
         "context": stage_ctx,
         "adapter_request": adapter_request,
         "payload": payload,
         "results": {},
         "job_id": stage_ctx.job_id,
+        "phase_index": phase_index,
+        "phase_ready": phase_ready,
     }
     logger.debug(
         "dagster.bootstrap.initialised",
         tenant_id=stage_ctx.tenant_id,
         pipeline=stage_ctx.pipeline_name,
     )
     return state


 def _stage_state_key(stage_type: str) -> str:
     return {
         "ingest": "payloads",
         "parse": "document",
         "ir-validation": "document",
         "chunk": "chunks",
         "embed": "embedding_batch",
         "index": "index_receipt",
         "extract": "extraction",
         "knowledge-graph": "graph_receipt",
     }.get(stage_type, stage_type)


 def _apply_stage_output(
     stage_type: str,
     stage_name: str,
     state: dict[str, Any],
     output: Any,
 ) -> dict[str, Any]:
-    if stage_type == "ingest":
+    if stage_type == "gate":
+        state.setdefault("gates", {})[stage_name] = output
+    elif stage_type == "ingest":
         state["payloads"] = output
     elif stage_type in {"parse", "ir-validation"}:
         state["document"] = output
     elif stage_type == "chunk":
         state["chunks"] = output
     elif stage_type == "embed":
         state["embedding_batch"] = output
     elif stage_type == "index":
         state["index_receipt"] = output
     elif stage_type == "extract":
         entities, claims = output
         state["entities"] = entities
         state["claims"] = claims
     elif stage_type == "knowledge-graph":
         state["graph_receipt"] = output
     else:  # pragma: no cover - guard for future expansion
         state[_stage_state_key(stage_type)] = output
     state.setdefault("results", {})[stage_name] = {
         "type": stage_type,
         "output": state.get(_stage_state_key(stage_type)),
     }
     return state


 def _infer_output_count(stage_type: str, output: Any) -> int:
     if output is None:
         return 0
     if stage_type in {"ingest", "chunk"} and isinstance(output, Sequence):
         return len(output)
     if stage_type in {"parse", "ir-validation"}:
         return 1
+    if stage_type == "gate":
+        return 0
     if stage_type == "embed" and hasattr(output, "vectors"):
         vectors = getattr(output, "vectors")
         if isinstance(vectors, Sequence):
             return len(vectors)
     if stage_type == "index" and hasattr(output, "chunks_indexed"):
         indexed = getattr(output, "chunks_indexed")
         if isinstance(indexed, int):
             return indexed
     if stage_type == "extract" and isinstance(output, tuple) and len(output) == 2:
         entities, claims = output
         entity_count = len(entities) if isinstance(entities, Sequence) else 0
         claim_count = len(claims) if isinstance(claims, Sequence) else 0
         return entity_count + claim_count
     if stage_type == "knowledge-graph" and hasattr(output, "nodes_written"):
         nodes = getattr(output, "nodes_written", 0)
         if isinstance(nodes, int):
             return nodes
     return 1


 def _make_stage_op(
     topology: PipelineTopologyConfig,
     stage_definition: StageDefinition,
 ):
     stage_type = stage_definition.stage_type
     stage_name = stage_definition.name
     policy_name = stage_definition.policy or "default"

     @op(
         name=stage_name,
         ins={"state": In(dict)},
         out=Out(dict),
         required_resource_keys={
             "stage_factory",
             "resilience_policies",
             "job_ledger",
             "event_emitter",
         },
     )
     def _stage_op(context, state: dict[str, Any]) -> dict[str, Any]:
-        stage = context.resources.stage_factory.resolve(topology.name, stage_definition)
+        stage_factory: StageFactory = context.resources.stage_factory
+        stage = stage_factory.resolve(topology, stage_definition)
         policy_loader: ResiliencePolicyLoader = context.resources.resilience_policies

         execute = getattr(stage, "execute")
         execution_state: dict[str, Any] = {
             "attempts": 0,
             "duration": 0.0,
             "failed": False,
             "error": None,
         }

         def _on_retry(retry_state: Any) -> None:
             job_identifier = state.get("job_id")
             if job_identifier:
                 ledger.increment_retry(job_identifier, stage_name)
             sleep_seconds = getattr(getattr(retry_state, "next_action", None), "sleep", 0.0) or 0.0
             attempt_number = getattr(retry_state, "attempt_number", 0) + 1
             error = getattr(getattr(retry_state, "outcome", None), "exception", lambda: None)()
             reason = str(error) if error else "retry"
             emitter.emit_retrying(
                 state["context"],
                 stage_name,
                 attempt=attempt_number,
                 backoff_ms=int(sleep_seconds * 1000),
                 reason=reason,
             )

         def _on_success(attempts: int, duration: float) -> None:
             execution_state["attempts"] = attempts
             execution_state["duration"] = duration

         def _on_failure(error: BaseException, attempts: int) -> None:
             execution_state["attempts"] = attempts
             execution_state["failed"] = True
             execution_state["error"] = error

         hooks = StageExecutionHooks(
             on_retry=_on_retry,
             on_success=_on_success,
             on_failure=_on_failure,
         )

         wrapped = policy_loader.apply(policy_name, stage_name, execute, hooks=hooks)

         stage_ctx: StageContext = state["context"]
+        stage_ctx = stage_ctx.with_phase(stage_definition.execution_phase)
+        state["context"] = stage_ctx
         job_id = stage_ctx.job_id or state.get("job_id")

         initial_attempt = 1
         if job_id:
             entry = ledger.mark_stage_started(job_id, stage_name)
             initial_attempt = entry.retry_count_per_stage.get(stage_name, 0) + 1
         emitter.emit_started(stage_ctx, stage_name, attempt=initial_attempt)

         start_time = time.perf_counter()

+        current_phase_index = int(state.get("phase_index", 1))
+        target_phase_index = stage_definition.phase_index
+
+        if stage_definition.is_gate and target_phase_index < current_phase_index:
+            logger.debug(
+                "dagster.stage.skipped.gate",
+                pipeline=topology.name,
+                stage=stage_name,
+                reason="gate already satisfied",
+            )
+            return state
+
+        if not stage_definition.is_gate and target_phase_index > current_phase_index:
+            logger.debug(
+                "dagster.stage.skipped.locked_phase",
+                pipeline=topology.name,
+                stage=stage_name,
+                required_phase=target_phase_index,
+                current_phase=current_phase_index,
+            )
+            return state
+
+        if not stage_definition.is_gate and target_phase_index < current_phase_index:
+            logger.debug(
+                "dagster.stage.skipped.completed_phase",
+                pipeline=topology.name,
+                stage=stage_name,
+                phase_index=target_phase_index,
+                current_phase=current_phase_index,
+            )
+            return state
+
         try:
             if stage_type == "ingest":
                 adapter_request: AdapterRequest = state["adapter_request"]
                 result = wrapped(stage_ctx, adapter_request)
             elif stage_type in {"parse", "ir-validation"}:
                 payloads = state.get("payloads", [])
                 result = wrapped(stage_ctx, payloads)
             elif stage_type == "chunk":
                 document = state.get("document")
                 result = wrapped(stage_ctx, document)
             elif stage_type == "embed":
                 chunks = state.get("chunks", [])
                 result = wrapped(stage_ctx, chunks)
             elif stage_type == "index":
                 batch = state.get("embedding_batch")
                 result = wrapped(stage_ctx, batch)
             elif stage_type == "extract":
                 document = state.get("document")
                 result = wrapped(stage_ctx, document)
             elif stage_type == "knowledge-graph":
                 entities = state.get("entities", [])
                 claims = state.get("claims", [])
                 result = wrapped(stage_ctx, entities, claims)
+            elif stage_type == "gate":
+                result = wrapped(stage_ctx, state, ledger=ledger)
             else:  # pragma: no cover - guard for future expansion
                 upstream = state.get(_stage_state_key(stage_type))
                 result = wrapped(stage_ctx, upstream)
+        except GateConditionError as exc:
+            attempts = execution_state.get("attempts") or 1
+            logger.info(
+                "dagster.stage.gate_blocked",
+                pipeline=topology.name,
+                stage=stage_name,
+                gate=getattr(stage_definition, "gate", None),
+                status=exc.status,
+                attempts=attempts,
+                message=str(exc),
+            )
+            if job_id:
+                ledger.record_gate_state(
+                    job_id,
+                    getattr(stage_definition, "gate", stage_name),
+                    status=exc.status,
+                    reason=str(exc),
+                    attempts=attempts,
+                )
+                metadata_update = {
+                    "phase_index": stage_definition.phase_index,
+                    "phase_ready": False,
+                    f"gate.{getattr(stage_definition, 'gate', stage_name)}.status": exc.status,
+                    f"gate.{getattr(stage_definition, 'gate', stage_name)}.reason": str(exc),
+                }
+                resume_target = getattr(getattr(stage, "gate", None), "resume_stage", None)
+                if resume_target:
+                    metadata_update["resume_stage"] = resume_target
+                    metadata_update[
+                        f"gate.{getattr(stage_definition, 'gate', stage_name)}.resume_stage"
+                    ] = resume_target
+                ledger.update_metadata(job_id, metadata_update)
+            record_gate_evaluation(getattr(stage_definition, "gate", stage_name), exc.status)
+            state.setdefault("gates", {})[stage_name] = {
+                "status": exc.status,
+                "reason": str(exc),
+                "attempts": attempts,
+            }
+            state["phase_index"] = stage_definition.phase_index
+            state["phase_ready"] = False
+            state.setdefault("results", {})[stage_name] = {
+                "type": stage_type,
+                "output": state["gates"][stage_name],
+            }
+            return state
         except Exception as exc:
             attempts = execution_state.get("attempts") or 1
             emitter.emit_failed(stage_ctx, stage_name, attempt=attempts, error=str(exc))
             if job_id:
                 ledger.mark_failed(job_id, stage=stage_name, reason=str(exc))
             raise

         updated = dict(state)
-        _apply_stage_output(stage_type, stage_name, updated, result)
-        output = updated.get(_stage_state_key(stage_type))
+        if stage_type == "gate":
+            gate_output = {
+                "status": result.status,
+                "attempts": result.attempts,
+                "elapsed_seconds": result.elapsed_seconds,
+                "metadata": result.metadata,
+            }
+            gate_name = getattr(stage_definition, "gate", stage_name)
+            updated.setdefault("gates", {})[stage_name] = gate_output
+            updated.setdefault("results", {})[stage_name] = {
+                "type": stage_type,
+                "output": gate_output,
+            }
+            updated["phase_ready"] = result.should_resume
+            if result.should_resume:
+                updated["phase_index"] = stage_definition.phase_index + 1
+            else:
+                updated["phase_index"] = stage_definition.phase_index
+            if job_id:
+                metadata_update = {
+                    "phase_index": updated["phase_index"],
+                    "phase_ready": result.should_resume,
+                    f"gate.{gate_name}.status": result.status,
+                }
+                resume_target = getattr(getattr(stage, "gate", None), "resume_stage", None)
+                if resume_target:
+                    metadata_update["resume_stage"] = resume_target
+                    metadata_update[f"gate.{gate_name}.resume_stage"] = resume_target
+                ledger.update_metadata(job_id, metadata_update)
+            record_gate_evaluation(gate_name, result.status)
+            if result.should_resume and updated["phase_index"] != current_phase_index:
+                record_phase_transition(
+                    topology.name,
+                    f"phase-{current_phase_index}",
+                    f"phase-{updated['phase_index']}",
+                )
+            output = gate_output
+        else:
+            _apply_stage_output(stage_type, stage_name, updated, result)
+            output = updated.get(_stage_state_key(stage_type))
         attempts = execution_state.get("attempts") or 1
         duration_seconds = execution_state.get("duration") or (time.perf_counter() - start_time)
         duration_ms = int(duration_seconds * 1000)
         output_count = _infer_output_count(stage_type, output)

         if job_id:
             ledger.update_metadata(
                 job_id,
                 {
                     f"stage.{stage_name}.attempts": attempts,
                     f"stage.{stage_name}.output_count": output_count,
                     f"stage.{stage_name}.duration_ms": duration_ms,
                 },
             )
         emitter.emit_completed(
             stage_ctx,
             stage_name,
             attempt=attempts,
             duration_ms=duration_ms,
             output_count=output_count,
         )
         logger.debug(
             "dagster.stage.completed",
             pipeline=topology.name,
             stage=stage_name,
