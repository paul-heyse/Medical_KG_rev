# syntax=docker/dockerfile:1.5
ARG VLLM_BASE_IMAGE=vllm/vllm-openai:latest
FROM ${VLLM_BASE_IMAGE}

ARG MODEL_SOURCE_DIR=qwen3-embedding-8b
ARG MODEL_NAME=qwen3-embedding-8b
ARG MODEL_ID="Qwen/Qwen2.5-Coder-1.5B"
ARG GPU_MEMORY_UTILIZATION=0.9
ARG MAX_MODEL_LEN=8192

ENV MODEL_ID=${MODEL_ID} \
    MODEL_NAME=${MODEL_NAME} \
    MODEL_PATH=/models/${MODEL_NAME} \
    GPU_MEMORY_UTILIZATION=${GPU_MEMORY_UTILIZATION} \
    MAX_MODEL_LEN=${MAX_MODEL_LEN}

# The model artifacts are expected to be available in the build context under
# models/${MODEL_SOURCE_DIR}. They are mounted read-only inside the container so
# that runtime environments do not mutate shared checkpoints.
COPY models/${MODEL_SOURCE_DIR} ${MODEL_PATH}

CMD [
  "vllm",
  "serve",
  "${MODEL_PATH}",
  "--host", "0.0.0.0",
  "--port", "8001",
  "--gpu-memory-utilization", "${GPU_MEMORY_UTILIZATION}",
  "--max-model-len", "${MAX_MODEL_LEN}",
  "--served-model-name", "${MODEL_ID}"
]
