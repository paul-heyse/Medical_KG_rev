diff a/src/Medical_KG_rev/orchestration/dagster/runtime.py b/src/Medical_KG_rev/orchestration/dagster/runtime.py	(rejected hunks)
@@ -4,99 +4,282 @@ from __future__ import annotations

 from dataclasses import dataclass
 import re
 import time
 from pathlib import Path
 from typing import Any, Callable, Mapping, Sequence
 from uuid import uuid4

 from dagster import (
     Definitions,
     ExecuteInProcessResult,
     In,
     Out,
     ResourceDefinition,
     RunRequest,
     SensorEvaluationContext,
     SkipReason,
     graph,
     op,
     sensor,
 )

 from Medical_KG_rev.adapters.plugins.bootstrap import get_plugin_manager
 from Medical_KG_rev.adapters.plugins.manager import AdapterPluginManager
 from Medical_KG_rev.adapters.plugins.models import AdapterRequest
+from Medical_KG_rev.observability.metrics import (
+    record_gate_evaluation,
+    record_gate_timeout,
+    record_phase_transition,
+)
 from Medical_KG_rev.orchestration.dagster.configuration import (
+    GateConditionOperator,
+    GateDefinition,
     PipelineConfigLoader,
     PipelineTopologyConfig,
     StageExecutionHooks,
     ResiliencePolicyLoader,
     StageDefinition,
 )
 from Medical_KG_rev.orchestration.dagster.stages import (
     HaystackPipelineResource,
     build_default_stage_factory,
     create_default_pipeline_resource,
 )
 from Medical_KG_rev.orchestration.events import StageEventEmitter
 from Medical_KG_rev.orchestration.kafka import KafkaClient
-from Medical_KG_rev.orchestration.ledger import JobLedger, JobLedgerError
+from Medical_KG_rev.orchestration.ledger import JobLedger, JobLedgerEntry, JobLedgerError
 from Medical_KG_rev.orchestration.openlineage import OpenLineageEmitter
 from Medical_KG_rev.orchestration.stages.contracts import StageContext
 from Medical_KG_rev.utils.logging import get_logger

 logger = get_logger(__name__)


 class StageResolutionError(RuntimeError):
     """Raised when a stage cannot be resolved from the registry."""


 @dataclass(slots=True)
 class StageFactory:
     """Resolve orchestration stages by topology stage type."""

     registry: Mapping[str, Callable[[StageDefinition], object]]

     def resolve(self, pipeline: str, stage: StageDefinition) -> object:
         try:
             factory = self.registry[stage.stage_type]
         except KeyError as exc:  # pragma: no cover - defensive guard
             raise StageResolutionError(
                 f"Pipeline '{pipeline}' declared unknown stage type '{stage.stage_type}'"
             ) from exc
         instance = factory(stage)
         logger.debug(
             "dagster.stage.resolved",
             pipeline=pipeline,
             stage=stage.name,
             stage_type=stage.stage_type,
         )
         return instance


+class GateConditionError(RuntimeError):
+    """Raised when a gate condition fails or times out."""
+
+    def __init__(
+        self,
+        gate: str,
+        message: str,
+        *,
+        timeout: bool = False,
+        result: GateEvaluationResult | None = None,
+    ) -> None:
+        super().__init__(message)
+        self.gate = gate
+        self.timeout = timeout
+        self.result = result
+
+
+@dataclass(slots=True)
+class GateEvaluationResult:
+    """Outcome of a gate evaluation."""
+
+    gate: str
+    satisfied: bool
+    attempts: int
+    duration_seconds: float
+    observed: Mapping[str, Any]
+    resume_stage: str | None = None
+    last_error: str | None = None
+
+
+class GateConditionEvaluator:
+    """Evaluate gate predicates against ledger entries."""
+
+    def __init__(self, definition: GateDefinition) -> None:
+        self._definition = definition
+
+    def evaluate(
+        self,
+        entry: JobLedgerEntry,
+        *,
+        previous_observed: Mapping[str, Any] | None = None,
+    ) -> tuple[bool, dict[str, Any]]:
+        observed: dict[str, Any] = {}
+        previous_observed = previous_observed or {}
+        satisfied: list[bool] = []
+        for clause in self._definition.condition.clauses:
+            value = self._resolve_field(entry, clause.field)
+            observed[clause.field] = value
+            if clause.operator is GateConditionOperator.EQUALS:
+                satisfied.append(value == clause.value)
+            elif clause.operator is GateConditionOperator.EXISTS:
+                satisfied.append(value is not None)
+            elif clause.operator is GateConditionOperator.CHANGED:
+                satisfied.append(previous_observed.get(clause.field) != value)
+            else:  # pragma: no cover - defensive guard
+                satisfied.append(False)
+        if self._definition.condition.logic == "any":
+            overall = any(satisfied)
+        else:
+            overall = all(satisfied)
+        return overall, observed
+
+    @staticmethod
+    def _resolve_field(entry: JobLedgerEntry, field: str) -> Any:
+        target: Any = entry
+        for part in field.split('.'):
+            if isinstance(target, Mapping):
+                target = target.get(part)
+            else:
+                target = getattr(target, part, None)
+            if target is None:
+                break
+        return target
+
+
+class GateStage:
+    """Runtime executor for gate stages."""
+
+    def __init__(
+        self,
+        gate: GateDefinition,
+        *,
+        ledger: JobLedger,
+        evaluator: GateConditionEvaluator,
+        sleep: Callable[[float], None] = time.sleep,
+    ) -> None:
+        self._gate = gate
+        self._ledger = ledger
+        self._evaluator = evaluator
+        self._sleep = sleep
+
+    def execute(self, ctx: StageContext, state: dict[str, Any]) -> GateEvaluationResult:
+        job_id = ctx.job_id or state.get("job_id")
+        if not job_id:
+            raise GateConditionError(self._gate.name, "Gate stages require a job identifier")
+
+        attempts = 0
+        start = time.perf_counter()
+        overall_deadline = None
+        if self._gate.timeout_seconds:
+            overall_deadline = time.monotonic() + float(self._gate.timeout_seconds)
+        previous_result = (state.get("gate_results", {}) or {}).get(self._gate.name, {})
+        last_error: str | None = None
+        last_observed: dict[str, Any] = dict(previous_result.get("observed") or {})
+        timeout_triggered = False
+
+        retry_config = self._gate.retry
+        max_attempts = retry_config.max_attempts if retry_config else 1
+        backoff = retry_config.backoff_seconds if retry_config else 0.0
+        condition_timeout = self._gate.condition.timeout_seconds
+        poll_interval = self._gate.condition.poll_interval_seconds
+
+        while attempts < max_attempts:
+            attempts += 1
+            attempt_start = time.monotonic()
+            while True:
+                now = time.monotonic()
+                if overall_deadline and now >= overall_deadline:
+                    timeout_triggered = True
+                    last_error = "Gate evaluation exceeded overall timeout"
+                    break
+                entry = self._ledger.get(job_id)
+                if entry is None:
+                    raise GateConditionError(self._gate.name, f"Job {job_id} not found in ledger")
+                satisfied, observed = self._evaluator.evaluate(
+                    entry,
+                    previous_observed=previous_result.get("observed"),
+                )
+                last_observed = dict(observed)
+                previous_result = {"observed": last_observed}
+                logger.debug(
+                    "dagster.gate.evaluate",
+                    gate=self._gate.name,
+                    job_id=job_id,
+                    satisfied=satisfied,
+                    attempts=attempts,
+                )
+                if satisfied:
+                    duration = time.perf_counter() - start
+                    result = GateEvaluationResult(
+                        gate=self._gate.name,
+                        satisfied=True,
+                        attempts=attempts,
+                        duration_seconds=duration,
+                        observed=observed,
+                        resume_stage=self._gate.resume_stage,
+                    )
+                    return result
+                if condition_timeout and now - attempt_start >= condition_timeout:
+                    timeout_triggered = True
+                    last_error = "Gate condition timed out"
+                    break
+                self._sleep(poll_interval)
+            if timeout_triggered or attempts >= max_attempts:
+                break
+            if backoff:
+                self._sleep(backoff)
+
+        duration = time.perf_counter() - start
+        result = GateEvaluationResult(
+            gate=self._gate.name,
+            satisfied=False,
+            attempts=attempts,
+            duration_seconds=duration,
+            observed=last_observed,
+            resume_stage=self._gate.resume_stage,
+            last_error=last_error or "Gate condition not satisfied",
+        )
+        raise GateConditionError(
+            self._gate.name,
+            result.last_error or "Gate failed",
+            timeout=timeout_triggered,
+            result=result,
+        )
+
+
 @op(
     name="bootstrap",
     out=Out(dict),
     config_schema={
         "context": dict,
         "adapter_request": dict,
         "payload": dict,
     },
 )
 def bootstrap_op(context) -> dict[str, Any]:
     """Initialise the orchestration state for a Dagster run."""

     ctx_payload = context.op_config["context"]
     adapter_payload = context.op_config["adapter_request"]
     payload = context.op_config.get("payload", {})

     stage_ctx = StageContext(
         tenant_id=ctx_payload["tenant_id"],
         job_id=ctx_payload.get("job_id"),
         doc_id=ctx_payload.get("doc_id"),
         correlation_id=ctx_payload.get("correlation_id"),
         metadata=ctx_payload.get("metadata", {}),
         pipeline_name=ctx_payload.get("pipeline_name"),
         pipeline_version=ctx_payload.get("pipeline_version"),
     )
@@ -108,132 +291,229 @@ def bootstrap_op(context) -> dict[str, Any]:
         "payload": payload,
         "results": {},
         "job_id": stage_ctx.job_id,
     }
     logger.debug(
         "dagster.bootstrap.initialised",
         tenant_id=stage_ctx.tenant_id,
         pipeline=stage_ctx.pipeline_name,
     )
     return state


 def _stage_state_key(stage_type: str) -> str:
     return {
         "ingest": "payloads",
         "parse": "document",
         "ir-validation": "document",
         "chunk": "chunks",
         "embed": "embedding_batch",
         "index": "index_receipt",
         "extract": "extraction",
         "knowledge-graph": "graph_receipt",
     }.get(stage_type, stage_type)


+def _resolve_gate_definition(
+    topology: PipelineTopologyConfig, stage: StageDefinition
+) -> GateDefinition:
+    gate_name = stage.gate_name or stage.name
+    for definition in topology.gates:
+        if definition.name == gate_name:
+            return definition
+    raise ValueError(f"No gate definition found for stage '{stage.name}'")
+
+
 def _apply_stage_output(
     stage_type: str,
     stage_name: str,
     state: dict[str, Any],
     output: Any,
 ) -> dict[str, Any]:
     if stage_type == "ingest":
         state["payloads"] = output
     elif stage_type in {"parse", "ir-validation"}:
         state["document"] = output
     elif stage_type == "chunk":
         state["chunks"] = output
     elif stage_type == "embed":
         state["embedding_batch"] = output
     elif stage_type == "index":
         state["index_receipt"] = output
     elif stage_type == "extract":
         entities, claims = output
         state["entities"] = entities
         state["claims"] = claims
     elif stage_type == "knowledge-graph":
         state["graph_receipt"] = output
     else:  # pragma: no cover - guard for future expansion
         state[_stage_state_key(stage_type)] = output
     state.setdefault("results", {})[stage_name] = {
         "type": stage_type,
         "output": state.get(_stage_state_key(stage_type)),
     }
     return state


+def _apply_gate_result(
+    topology: PipelineTopologyConfig,
+    stage_definition: StageDefinition,
+    state: dict[str, Any],
+    result: GateEvaluationResult,
+    phase_map: Mapping[str, int],
+) -> dict[str, Any]:
+    updated = dict(state)
+    gate_results = {**(updated.get("gate_results") or {})}
+    gate_results[result.gate] = {
+        "satisfied": result.satisfied,
+        "attempts": result.attempts,
+        "duration_seconds": result.duration_seconds,
+        "resume_stage": result.resume_stage,
+        "resume_phase": phase_map.get(result.resume_stage),
+        "observed": dict(result.observed),
+        "last_error": result.last_error,
+        "recorded_at": time.time(),
+    }
+    updated["gate_results"] = gate_results
+
+    phases = dict(updated.get("phases") or {})
+    map_snapshot = dict(phases.get("map") or {})
+    if not map_snapshot:
+        map_snapshot = dict(phase_map)
+    phases.setdefault("map", map_snapshot)
+    phases.setdefault("history", [])
+    phases.setdefault("completed", [])
+    phases.setdefault("transitions", [])
+    current_phase = phase_map.get(stage_definition.name, phases.get("current", 1))
+    phases["current"] = current_phase
+    pending = list(phases.get("pending", []))
+    if result.satisfied:
+        if result.gate in pending:
+            pending = [gate for gate in pending if gate != result.gate]
+        if result.gate not in phases["completed"]:
+            phases["completed"].append(result.gate)
+        phases["history"].append(
+            {
+                "gate": result.gate,
+                "phase": current_phase,
+                "timestamp": time.time(),
+            }
+        )
+        next_phase = phase_map.get(result.resume_stage, current_phase + 1)
+        phases["current"] = next_phase
+        label = f"phase-{next_phase}"
+        if label not in phases["transitions"]:
+            phases["transitions"].append(label)
+            record_phase_transition(topology.name, label)
+    else:
+        if result.gate not in pending:
+            pending.append(result.gate)
+    phases["pending"] = pending
+    updated["phases"] = phases
+    updated.setdefault("results", {})[stage_definition.name] = {
+        "type": "gate",
+        "output": None,
+        "satisfied": result.satisfied,
+    }
+    return updated
+
+
 def _infer_output_count(stage_type: str, output: Any) -> int:
     if output is None:
         return 0
     if stage_type in {"ingest", "chunk"} and isinstance(output, Sequence):
         return len(output)
     if stage_type in {"parse", "ir-validation"}:
         return 1
     if stage_type == "embed" and hasattr(output, "vectors"):
         vectors = getattr(output, "vectors")
         if isinstance(vectors, Sequence):
             return len(vectors)
     if stage_type == "index" and hasattr(output, "chunks_indexed"):
         indexed = getattr(output, "chunks_indexed")
         if isinstance(indexed, int):
             return indexed
     if stage_type == "extract" and isinstance(output, tuple) and len(output) == 2:
         entities, claims = output
         entity_count = len(entities) if isinstance(entities, Sequence) else 0
         claim_count = len(claims) if isinstance(claims, Sequence) else 0
         return entity_count + claim_count
     if stage_type == "knowledge-graph" and hasattr(output, "nodes_written"):
         nodes = getattr(output, "nodes_written", 0)
         if isinstance(nodes, int):
             return nodes
     return 1


 def _make_stage_op(
     topology: PipelineTopologyConfig,
     stage_definition: StageDefinition,
+    phase_map: Mapping[str, int],
 ):
     stage_type = stage_definition.stage_type
     stage_name = stage_definition.name
     policy_name = stage_definition.policy or "default"
+    stage_phase = phase_map.get(stage_name, 1)

     @op(
         name=stage_name,
         ins={"state": In(dict)},
         out=Out(dict),
         required_resource_keys={
             "stage_factory",
             "resilience_policies",
             "job_ledger",
             "event_emitter",
         },
     )
     def _stage_op(context, state: dict[str, Any]) -> dict[str, Any]:
-        stage = context.resources.stage_factory.resolve(topology.name, stage_definition)
         policy_loader: ResiliencePolicyLoader = context.resources.resilience_policies
-
-        execute = getattr(stage, "execute")
+        stage_factory: StageFactory = context.resources.stage_factory
+        ledger: JobLedger = context.resources.job_ledger
+        emitter: StageEventEmitter = context.resources.event_emitter
+
+        phases_state = state.get("phases", {}) or {}
+        current_phase = phases_state.get("current", 1)
+        if stage_phase < current_phase:
+            logger.debug(
+                "dagster.stage.skipped",
+                pipeline=topology.name,
+                stage=stage_name,
+                reason="phase completed",
+                phase=stage_phase,
+                current_phase=current_phase,
+            )
+            return state
+
+        if stage_type == "gate":
+            gate_definition = _resolve_gate_definition(topology, stage_definition)
+            evaluator = GateConditionEvaluator(gate_definition)
+            gate_stage = GateStage(gate_definition, ledger=ledger, evaluator=evaluator)
+            execute = gate_stage.execute
+        else:
+            stage = stage_factory.resolve(topology.name, stage_definition)
+            execute = getattr(stage, "execute")
         execution_state: dict[str, Any] = {
             "attempts": 0,
             "duration": 0.0,
             "failed": False,
             "error": None,
         }

         def _on_retry(retry_state: Any) -> None:
             job_identifier = state.get("job_id")
             if job_identifier:
                 ledger.increment_retry(job_identifier, stage_name)
             sleep_seconds = getattr(getattr(retry_state, "next_action", None), "sleep", 0.0) or 0.0
             attempt_number = getattr(retry_state, "attempt_number", 0) + 1
             error = getattr(getattr(retry_state, "outcome", None), "exception", lambda: None)()
             reason = str(error) if error else "retry"
             emitter.emit_retrying(
                 state["context"],
                 stage_name,
                 attempt=attempt_number,
                 backoff_ms=int(sleep_seconds * 1000),
                 reason=reason,
             )

         def _on_success(attempts: int, duration: float) -> None:
             execution_state["attempts"] = attempts
@@ -242,99 +522,175 @@ def _make_stage_op(
         def _on_failure(error: BaseException, attempts: int) -> None:
             execution_state["attempts"] = attempts
             execution_state["failed"] = True
             execution_state["error"] = error

         hooks = StageExecutionHooks(
             on_retry=_on_retry,
             on_success=_on_success,
             on_failure=_on_failure,
         )

         wrapped = policy_loader.apply(policy_name, stage_name, execute, hooks=hooks)

         stage_ctx: StageContext = state["context"]
         job_id = stage_ctx.job_id or state.get("job_id")

         initial_attempt = 1
         if job_id:
             entry = ledger.mark_stage_started(job_id, stage_name)
             initial_attempt = entry.retry_count_per_stage.get(stage_name, 0) + 1
         emitter.emit_started(stage_ctx, stage_name, attempt=initial_attempt)

         start_time = time.perf_counter()

         try:
-            if stage_type == "ingest":
+            if stage_type == "gate":
+                result = wrapped(stage_ctx, state)
+            elif stage_type == "ingest":
                 adapter_request: AdapterRequest = state["adapter_request"]
                 result = wrapped(stage_ctx, adapter_request)
             elif stage_type in {"parse", "ir-validation"}:
                 payloads = state.get("payloads", [])
                 result = wrapped(stage_ctx, payloads)
             elif stage_type == "chunk":
                 document = state.get("document")
                 result = wrapped(stage_ctx, document)
             elif stage_type == "embed":
                 chunks = state.get("chunks", [])
                 result = wrapped(stage_ctx, chunks)
             elif stage_type == "index":
                 batch = state.get("embedding_batch")
                 result = wrapped(stage_ctx, batch)
             elif stage_type == "extract":
                 document = state.get("document")
                 result = wrapped(stage_ctx, document)
             elif stage_type == "knowledge-graph":
                 entities = state.get("entities", [])
                 claims = state.get("claims", [])
                 result = wrapped(stage_ctx, entities, claims)
             else:  # pragma: no cover - guard for future expansion
                 upstream = state.get(_stage_state_key(stage_type))
                 result = wrapped(stage_ctx, upstream)
+        except GateConditionError as exc:
+            attempts = execution_state.get("attempts") or 1
+            emitter.emit_failed(stage_ctx, stage_name, attempt=attempts, error=str(exc))
+            if job_id:
+                metadata: dict[str, Any] = {
+                    f"stage.{stage_name}.gate.status": "timeout"
+                    if exc.timeout
+                    else "waiting",
+                    f"stage.{stage_name}.gate.error": str(exc),
+                }
+                if exc.result:
+                    metadata[f"stage.{stage_name}.gate.attempts"] = exc.result.attempts
+                    metadata[f"gate.{exc.result.gate}.status"] = (
+                        "timeout" if exc.timeout else "waiting"
+                    )
+                    metadata[f"gate.{exc.result.gate}.resume_stage"] = exc.result.resume_stage
+                    metadata[f"gate.{exc.result.gate}.attempts"] = exc.result.attempts
+                    metadata[f"gate.{exc.result.gate}.resume_phase"] = phase_map.get(
+                        exc.result.resume_stage, stage_phase + 1
+                    )
+                    if exc.result.last_error:
+                        metadata[f"gate.{exc.result.gate}.error"] = exc.result.last_error
+                ledger.update_metadata(job_id, metadata)
+            if exc.timeout:
+                record_gate_timeout(exc.gate)
+            if exc.result:
+                record_gate_evaluation(
+                    exc.result.gate,
+                    "timeout" if exc.timeout else "failure",
+                    exc.result.duration_seconds,
+                    attempts=exc.result.attempts,
+                )
+            else:
+                record_gate_evaluation(exc.gate, "failure", 0.0, attempts=attempts)
+            raise
         except Exception as exc:
             attempts = execution_state.get("attempts") or 1
             emitter.emit_failed(stage_ctx, stage_name, attempt=attempts, error=str(exc))
             if job_id:
                 ledger.mark_failed(job_id, stage=stage_name, reason=str(exc))
             raise

         updated = dict(state)
-        _apply_stage_output(stage_type, stage_name, updated, result)
-        output = updated.get(_stage_state_key(stage_type))
+        gate_result: GateEvaluationResult | None = None
+        if stage_type == "gate":
+            gate_result = result
+            updated = _apply_gate_result(
+                topology,
+                stage_definition,
+                updated,
+                gate_result,
+                phase_map,
+            )
+            output = None
+        else:
+            _apply_stage_output(stage_type, stage_name, updated, result)
+            output = updated.get(_stage_state_key(stage_type))
         attempts = execution_state.get("attempts") or 1
         duration_seconds = execution_state.get("duration") or (time.perf_counter() - start_time)
         duration_ms = int(duration_seconds * 1000)
-        output_count = _infer_output_count(stage_type, output)
+        output_count = 0 if stage_type == "gate" else _infer_output_count(stage_type, output)

         if job_id:
             ledger.update_metadata(
                 job_id,
                 {
                     f"stage.{stage_name}.attempts": attempts,
                     f"stage.{stage_name}.output_count": output_count,
                     f"stage.{stage_name}.duration_ms": duration_ms,
+                    **(
+                        {
+                            f"stage.{stage_name}.gate.status": "passed"
+                            if gate_result and gate_result.satisfied
+                            else "waiting",
+                            f"stage.{stage_name}.gate.resume": gate_result.resume_stage
+                            if gate_result
+                            else None,
+                            f"gate.{gate_result.gate}.status": "passed"
+                            if gate_result and gate_result.satisfied
+                            else "waiting",
+                            f"gate.{gate_result.gate}.resume_stage": gate_result.resume_stage,
+                            f"gate.{gate_result.gate}.attempts": gate_result.attempts,
+                            f"gate.{gate_result.gate}.resume_phase": phase_map.get(
+                                gate_result.resume_stage, stage_phase + 1
+                            ),
+                        }
+                        if gate_result
+                        else {}
+                    ),
                 },
             )
+        if gate_result:
+            record_gate_evaluation(
+                gate_result.gate,
+                "success" if gate_result.satisfied else "failure",
+                gate_result.duration_seconds,
+                attempts=gate_result.attempts,
+            )
         emitter.emit_completed(
             stage_ctx,
             stage_name,
             attempt=attempts,
             duration_ms=duration_ms,
             output_count=output_count,
         )
         logger.debug(
             "dagster.stage.completed",
             pipeline=topology.name,
             stage=stage_name,
             stage_type=stage_type,
             policy=policy_name,
             attempts=attempts,
             duration_ms=duration_ms,
             output_count=output_count,
         )
         return updated

     return _stage_op


 def _topological_order(stages: list[StageDefinition]) -> list[str]:
     graph: dict[str, set[str]] = {stage.name: set(stage.depends_on) for stage in stages}
     resolved: list[str] = []
@@ -342,100 +698,168 @@ def _topological_order(stages: list[StageDefinition]) -> list[str]:
     permanent: set[str] = set()

     def visit(node: str) -> None:
         if node in permanent:
             return
         if node in temporary:
             raise ValueError(f"Cycle detected involving stage '{node}'")
         temporary.add(node)
         for dep in graph.get(node, set()):
             visit(dep)
         temporary.remove(node)
         permanent.add(node)
         resolved.append(node)

     for stage in graph:
         visit(stage)
     return resolved


 @dataclass(slots=True)
 class BuiltPipelineJob:
     job_name: str
     job_definition: Any
     final_node: str
     version: str
+    topology: PipelineTopologyConfig
+    phase_map: Mapping[str, int]


 def _normalise_name(name: str) -> str:
     """Return a Dagster-safe identifier derived from the pipeline name."""

     candidate = re.sub(r"[^0-9A-Za-z_]+", "_", name)
     if not candidate:
         return "pipeline"
     if candidate[0].isdigit():
         candidate = f"p_{candidate}"
     return candidate


 def _build_pipeline_job(
     topology: PipelineTopologyConfig,
     *,
     resource_defs: Mapping[str, ResourceDefinition],
 ) -> BuiltPipelineJob:
+    order = _topological_order(topology.stages)
+    stage_lookup = {stage.name: stage for stage in topology.stages}
+    phase_map: dict[str, int] = {}
+    phase_index = 1
+    for stage_name in order:
+        definition = stage_lookup[stage_name]
+        phase_map[stage_name] = phase_index
+        if definition.stage_type == "gate":
+            phase_index += 1
+    for stage_name in order:
+        stage_phase = phase_map[stage_name]
+        for dependency in stage_lookup[stage_name].depends_on:
+            dep_phase = phase_map.get(dependency, stage_phase)
+            if dep_phase > stage_phase:
+                raise ValueError(
+                    f"stage '{stage_name}' depends on future stage '{dependency}' across gate boundary"
+                )
     stage_ops = {
-        stage.name: _make_stage_op(topology, stage)
+        stage.name: _make_stage_op(topology, stage, phase_map)
         for stage in topology.stages
     }
-    order = _topological_order(topology.stages)
+    phase_groups: dict[int, list[str]] = {}
+    for stage_name in order:
+        phase_groups.setdefault(phase_map[stage_name], []).append(stage_name)

     safe_name = _normalise_name(topology.name)

+    @op(name="initialise_state", ins={"state": In(dict)}, out=Out(dict))
+    def _initialise_state_op(state: dict[str, Any]) -> dict[str, Any]:
+        updated = dict(state)
+        phases = dict(updated.get("phases") or {})
+        phases.setdefault("current", 1)
+        phases.setdefault("map", dict(phase_map))
+        phases.setdefault("completed", [])
+        phases.setdefault("history", [])
+        phases.setdefault("transitions", [])
+        payload = updated.get("payload") or {}
+        resume_phase = payload.get("resume_phase")
+        resume_stage = payload.get("resume_stage")
+        if resume_phase is None and resume_stage:
+            resume_phase = phase_map.get(str(resume_stage))
+        if isinstance(resume_phase, str) and resume_phase.isdigit():
+            resume_phase = int(resume_phase)
+        if isinstance(resume_phase, int) and resume_phase >= 1:
+            phases["current"] = resume_phase
+        updated["phases"] = phases
+        gate_results = dict(updated.get("gate_results") or {})
+        gate_results_payload = payload.get("gate_results") or {}
+        if isinstance(gate_results_payload, Mapping):
+            gate_results.update(gate_results_payload)  # type: ignore[arg-type]
+        updated["gate_results"] = gate_results
+        return updated
+
+    def _make_phase_graph(phase: int, stages_for_phase: list[str]):
+        graph_name = f"{safe_name}_phase_{phase}"
+
+        @graph(name=graph_name)
+        def _phase_graph(state):
+            current = state
+            for stage_name in stages_for_phase:
+                op_def = stage_ops[stage_name].alias(stage_name)
+                current = op_def(current)
+            return current
+
+        return _phase_graph
+
+    phase_graphs = {
+        phase: _make_phase_graph(phase, stage_names)
+        for phase, stage_names in phase_groups.items()
+    }
+
     @graph(name=f"{safe_name}_graph")
     def _pipeline_graph():
         state = bootstrap_op.alias("bootstrap")()
-        for stage_name in order:
-            op_def = stage_ops[stage_name].alias(stage_name)
-            state = op_def(state)
+        state = _initialise_state_op.alias("initialise_state")(state)
+        for phase in sorted(phase_graphs):
+            phase_node = phase_graphs[phase].alias(f"phase_{phase}")
+            state = phase_node(state)
         return state

     job = _pipeline_graph.to_job(
         name=f"{safe_name}_job",
         resource_defs={
             **resource_defs,
         },
         tags={
             "medical_kg.pipeline": topology.name,
             "medical_kg.pipeline_version": topology.version,
         },
     )

     return BuiltPipelineJob(
         job_name=job.name,
         job_definition=job,
         final_node=order[-1] if order else "bootstrap",
         version=topology.version,
+        topology=topology,
+        phase_map=phase_map,
     )


 @dataclass(slots=True)
 class DagsterRunResult:
     """Result returned after executing a Dagster job."""

     pipeline: str
     success: bool
     state: dict[str, Any]
     dagster_result: ExecuteInProcessResult


 class DagsterOrchestrator:
     """Submit orchestration jobs to Dagster using declarative topology configs."""

     def __init__(
         self,
         pipeline_loader: PipelineConfigLoader,
         resilience_loader: ResiliencePolicyLoader,
         stage_factory: StageFactory,
         *,
         plugin_manager: AdapterPluginManager | None = None,
         job_ledger: JobLedger | None = None,
         kafka_client: KafkaClient | None = None,
@@ -618,85 +1042,119 @@ class DagsterOrchestrator:
 def submit_to_dagster(
     orchestrator: DagsterOrchestrator,
     *,
     pipeline: str,
     context: StageContext,
     adapter_request: AdapterRequest,
     payload: Mapping[str, Any] | None = None,
 ) -> DagsterRunResult:
     """Convenience helper mirroring the legacy orchestration API."""

     return orchestrator.submit(
         pipeline=pipeline,
         context=context,
         adapter_request=adapter_request,
         payload=payload or {},
     )


 @sensor(name="pdf_ir_ready_sensor", minimum_interval_seconds=30, required_resource_keys={"job_ledger"})
 def pdf_ir_ready_sensor(context: SensorEvaluationContext):
     ledger: JobLedger = context.resources.job_ledger
     ready_requests: list[RunRequest] = []
     for entry in ledger.all():
         if entry.pipeline_name != "pdf-two-phase":
             continue
-        if not entry.pdf_ir_ready or entry.status != "processing":
+        gate_status = entry.metadata.get("gate.pdf_ir_ready.status")
+        resume_stage = entry.metadata.get("gate.pdf_ir_ready.resume_stage")
+        resume_phase = entry.metadata.get("gate.pdf_ir_ready.resume_phase")
+        resume_requested = entry.metadata.get("gate.pdf_ir_ready.resume_requested")
+        if not entry.pdf_ir_ready or entry.status not in {"processing", "queued"}:
+            continue
+        if gate_status != "passed" or not resume_stage or resume_requested:
             continue
+        try:
+            parsed_phase = int(resume_phase) if resume_phase is not None else None
+        except (TypeError, ValueError):  # pragma: no cover - defensive guard
+            parsed_phase = None
+
         run_key = f"{entry.job_id}-resume"
         context_payload = {
             "tenant_id": entry.tenant_id,
             "job_id": entry.job_id,
             "doc_id": entry.doc_key,
             "correlation_id": entry.metadata.get("correlation_id"),
             "metadata": dict(entry.metadata),
             "pipeline_name": entry.pipeline_name,
-            "pipeline_version": entry.metadata.get("pipeline_version", entry.pipeline_name or ""),
+            "pipeline_version": entry.metadata.get(
+                "pipeline_version", entry.pipeline_name or ""
+            ),
         }
         adapter_payload = entry.metadata.get("adapter_request", {})
-        payload = entry.metadata.get("payload", {})
+        payload = dict(entry.metadata.get("payload", {}) or {})
+        gate_results = dict(payload.get("gate_results") or {})
+        gate_results.setdefault(
+            "pdf_ir_ready",
+            {
+                "satisfied": True,
+                "resume_stage": resume_stage,
+                "resume_phase": parsed_phase,
+            },
+        )
+        payload.update(
+            {
+                "resume_stage": resume_stage,
+                "resume_phase": parsed_phase,
+                "gate_results": gate_results,
+            }
+        )
         run_config = {
             "ops": {
                 "bootstrap": {
                     "config": {
                         "context": context_payload,
                         "adapter_request": adapter_payload,
                         "payload": payload,
                     }
                 }
             }
         }
         ready_requests.append(
             RunRequest(
                 run_key=run_key,
                 run_config=run_config,
                 tags={
                     "medical_kg.pipeline": entry.pipeline_name or "",
-                    "medical_kg.resume_stage": "chunk",
+                    "medical_kg.resume_stage": resume_stage,
+                    "medical_kg.resume_phase": str(parsed_phase) if parsed_phase else "",
                 },
             )
         )
+        ledger.update_metadata(
+            entry.job_id,
+            {f"gate.pdf_ir_ready.resume_requested": True},
+        )
     if not ready_requests:
         yield SkipReason("No PDF ingestion jobs ready for resumption")
         return
     for request in ready_requests:
         yield request


 def build_default_orchestrator() -> DagsterOrchestrator:
     """Construct a Dagster orchestrator with default stage builders."""

     pipeline_loader = PipelineConfigLoader()
     resilience_loader = ResiliencePolicyLoader()
     plugin_manager = get_plugin_manager()
     pipeline_resource = create_default_pipeline_resource()
     stage_builders = build_default_stage_factory(plugin_manager, pipeline_resource)
     stage_factory = StageFactory(stage_builders)
     job_ledger = JobLedger()
     kafka_client = KafkaClient()
     event_emitter = StageEventEmitter(kafka_client)
     openlineage_emitter = OpenLineageEmitter()
     return DagsterOrchestrator(
         pipeline_loader,
         resilience_loader,
         stage_factory,
         plugin_manager=plugin_manager,
