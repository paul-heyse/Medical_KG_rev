FROM vllm/vllm-openai:latest

COPY models/qwen3-embedding-8b /models/qwen3-embedding-8b

ENV MODEL_PATH=/models/qwen3-embedding-8b \
    GPU_MEMORY_UTILIZATION=0.9 \
    MAX_MODEL_LEN=8192

CMD [
  "vllm",
  "serve",
  "${MODEL_PATH}",
  "--host", "0.0.0.0",
  "--port", "8001",
  "--gpu-memory-utilization", "${GPU_MEMORY_UTILIZATION}",
  "--max-model-len", "${MAX_MODEL_LEN}"
]
