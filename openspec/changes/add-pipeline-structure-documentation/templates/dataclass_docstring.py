"""Dataclass docstring template for pipeline data models.

This template shows the required structure and content for dataclass docstrings
in the Medical_KG_rev pipeline codebase.
"""

# Example dataclass docstring structure:

"""[One-line summary of dataclass purpose].

[Detailed explanation of what the dataclass represents, when it's used,
and how it fits into the data flow.]

Fields:
    field_name: [Type and description, including valid ranges/constraints]
        [Additional details for complex fields]
    optional_field: [Description and what None means]. Defaults to None.

Serialization:
    [Describe how the dataclass is serialized/deserialized]
    [Mention any special handling for JSON, YAML, etc.]

Example:
    >>> request = ChunkingRequest(
    ...     tenant_id="tenant1",
    ...     document_id="doc1",
    ...     text="Sample text"
    ... )
    >>> print(request.document_id)
    doc1
"""

# Real example for ChunkingRequest:

"""Request model for synchronous chunking operations.

ChunkingRequest represents a request to chunk a document synchronously.
It contains all the information needed to perform chunking including
document identification, text content, chunking strategy, and parameters.

This dataclass is used by ChunkingCoordinator to receive chunking requests
from gateway services and contains all the parameters needed for chunking
operations including strategy selection and chunk size configuration.

Fields:
    tenant_id: Unique tenant identifier for multi-tenancy support.
        Must be non-empty string matching tenant ID format.
    document_id: Unique identifier for the document being chunked.
        Used for tracking and result assembly. Must be non-empty string.
    text: Optional document text content. If None, text will be
        extracted from options["text"]. Either text or options["text"]
        must be provided and non-empty.
    strategy: Chunking strategy name (e.g., "section", "semantic").
        Defaults to "section" if not specified. Must be one of
        the strategies available in ChunkingService.
    chunk_size: Maximum tokens per chunk. Defaults to profile setting
        if not specified. Must be positive integer.
    overlap: Token overlap between adjacent chunks. Defaults to profile
        setting if not specified. Must be non-negative integer.
    options: Additional metadata and configuration options.
        May contain text if request.text is None, profile overrides,
        and other chunking-specific parameters.

Serialization:
    Serializes to JSON for REST API requests. All fields are optional
    except tenant_id and document_id. Text can be provided in either
    the text field or options["text"] for backwards compatibility.

Example:
    >>> request = ChunkingRequest(
    ...     tenant_id="tenant1",
    ...     document_id="doc1",
    ...     text="Sample document text for chunking.",
    ...     strategy="section",
    ...     chunk_size=512,
    ...     overlap=50
    ... )
    >>> print(f"Chunking {request.document_id} with {request.strategy} strategy")
    Chunking doc1 with section strategy
"""

# Real example for ChunkingResult:

"""Result model for synchronous chunking operations.

ChunkingResult represents the result of a completed chunking operation.
It contains the generated chunks, job metadata, and performance metrics.

This dataclass is returned by ChunkingCoordinator after successful
chunking operations and contains all the information needed by
gateway services to construct API responses.

Fields:
    job_id: Unique job identifier for tracking and correlation.
        Generated by JobLifecycleManager during job creation.
    duration_s: Time taken to complete the chunking operation in seconds.
        Includes text extraction, chunking, and result assembly.
    chunks: Sequence of DocumentChunk objects containing the chunked
        content with metadata, token counts, and chunk indices.
    metadata: Additional metadata about the chunking operation including
        chunk count, strategy used, and any relevant metrics.

Serialization:
    Serializes to JSON for REST API responses. Chunks are serialized
    as arrays of DocumentChunk objects with all their fields.

Example:
    >>> result = ChunkingResult(
    ...     job_id="job-123",
    ...     duration_s=0.45,
    ...     chunks=[chunk1, chunk2, chunk3],
    ...     metadata={"chunk_count": 3, "strategy": "section"}
    ... )
    >>> print(f"Job {result.job_id} completed in {result.duration_s:.2f}s")
    >>> print(f"Generated {len(result.chunks)} chunks")
    Job job-123 completed in 0.45s
    Generated 3 chunks
"""
