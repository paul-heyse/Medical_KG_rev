"""Manage Qwen3 embedding generation for dense retrieval pipelines.

This module provides Qwen3 embedding generation capabilities for dense retrieval
pipelines, supporting both gRPC-based and legacy in-process model loading modes
with comprehensive observability and error handling.

Key Responsibilities:
    - Bootstrap the Qwen3 gRPC client and expose synchronous embedding APIs
    - Preprocess contextualized text prior to embedding generation
    - Record operational metrics and surface failures via rich exceptions
    - Support feature flag-based migration between gRPC and in-process modes
    - Provide backward compatibility during torch isolation migration

Collaborators:
    - Upstream: Retrieval services and orchestrators requesting embeddings
    - Downstream: Qwen3GRPCClient that communicates with isolated GPU services
    - Configuration: Feature flags controlling gRPC vs in-process operation modes
    - Observability: Metrics systems for operational monitoring and debugging

Side Effects:
    - Emits Prometheus metrics for operational observability
    - Performs network I/O through gRPC when enabled
    - May load large ML models into memory when in-process mode is used
    - Updates circuit breaker state for resilience monitoring

Thread Safety:
    - Not thread-safe: Instances maintain client state and should be used from a single thread
    - Concurrent access may lead to race conditions in model loading and inference

Performance Characteristics:
    - Embedding latency dominated by remote service round-trips (gRPC mode)
    - Lightweight preprocessing and metrics emission locally
    - Memory usage scales with model size when in-process mode is enabled
    - Network overhead when gRPC mode is enabled

Note:
    This service supports two operational modes controlled by feature flags:
    - gRPC mode (recommended): Delegates to isolated GPU microservices
    - In-process mode (legacy): Loads models locally for backward compatibility

Example:
    >>> service = Qwen3Service()
    >>> embeddings = service.generate_embeddings(["sample text"])
    >>> print(f"Generated {len(embeddings)} embeddings")

"""

# ==============================================================================
# IMPORTS
# ==============================================================================

from __future__ import annotations

import logging
import time
from typing import Any

from pydantic import BaseModel, Field

from Medical_KG_rev.config.settings import get_settings
from Medical_KG_rev.services.clients.qwen3_grpc_client import (
    Qwen3GRPCClient,
    Qwen3ServiceUnavailableError,
)
from prometheus_client import Counter, Histogram

logger = logging.getLogger(__name__)


# ==============================================================================
# TYPE DEFINITIONS
# ==============================================================================

QWEN3_OPERATIONS_TOTAL = Counter(
    "qwen3_operations_total",
    "Total number of Qwen3 operations",
    ["operation", "status"],
)

QWEN3_PROCESSING_SECONDS = Histogram(
    "qwen3_processing_seconds",
    "Time spent processing Qwen3 operations",
    ["operation", "status"],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0],
)

QWEN3_EMBEDDING_DIMENSIONS = Histogram(
    "qwen3_embedding_dimensions",
    "Embedding dimensions generated by Qwen3",
    ["operation"],
    buckets=[512, 1024, 2048, 4096, 8192],
)

QWEN3_BATCH_SIZE = Histogram(
    "qwen3_batch_size",
    "Batch sizes processed by Qwen3",
    ["operation"],
    buckets=[1, 5, 10, 20, 50, 100],
)


# ==============================================================================
# DATA MODELS
# ==============================================================================

class Qwen3ProcessingError(Exception):
    """Raised when Qwen3 processing fails.

    This exception is raised when Qwen3 embedding generation encounters
    an error during processing, such as model loading failures, inference
    errors, or resource constraints.

    Attributes:
        message: Human-readable error description
        cause: Optional underlying exception that caused this error
        recoverable: Whether the error condition might be temporary

    Example:
        >>> try:
        ...     embeddings = service.generate_embeddings(texts)
        ... except Qwen3ProcessingError as e:
        ...     print(f"Embedding failed: {e.message}")

    """

    def __init__(self, message: str, cause: Exception | None = None, recoverable: bool = False):
        """Initialize Qwen3 processing error.

        Args:
            message: Human-readable error description
            cause: Optional underlying exception that caused this error
            recoverable: Whether the error condition might be temporary

        """
        super().__init__(message)
        self.message = message
        self.cause = cause
        self.recoverable = recoverable


class Qwen3Result(BaseModel):
    """Result returned by Qwen3 embedding generation.

    This dataclass represents the result of a single Qwen3 embedding operation,
    containing the generated embedding vector along with metadata about the
    processing operation.

    Attributes:
        chunk_id: Unique identifier of the processed text chunk. Used for
            correlation between input chunks and output embeddings.
        embedding: Dense embedding vector produced by the Qwen3 model. Contains
            floating-point values representing the semantic encoding of the input text.
        model_name: Name and version of the Qwen3 model used for embedding generation.
            Used for tracking model performance and debugging.
        preprocessing_version: Version identifier of the preprocessing logic applied
            to the input text before embedding generation.
        processing_time_seconds: Total time elapsed during embedding generation in seconds.
            Used for performance monitoring and optimization.
        gpu_memory_used_mb: GPU memory consumption reported by legacy in-process
            implementations. Not used in gRPC mode.

    Example:
        >>> result = Qwen3Result(
        ...     chunk_id="doc_001_chunk_0",
        ...     embedding=[0.1, 0.2, 0.3, ...],
        ...     model_name="Qwen/Qwen2.5-7B-Instruct",
        ...     preprocessing_version="v1.0",
        ...     processing_time_seconds=0.45
        ... )
        >>> print(f"Embedding dimension: {len(result.embedding)}")

    """

    chunk_id: str = Field(description="Unique identifier of the processed text chunk")
    embedding: list[float] = Field(description="Dense embedding vector from Qwen3 model")
    model_name: str = Field(description="Name and version of the Qwen3 model used")
    preprocessing_version: str = Field(description="Version of preprocessing logic applied")
    processing_time_seconds: float = Field(description="Total processing time in seconds")
    gpu_memory_used_mb: float = Field(default=0.0, description="GPU memory consumption (legacy mode)")


# ==============================================================================
# IMPLEMENTATIONS
# ==============================================================================

class Qwen3Service:
    """Coordinate Qwen3 embedding generation using a gRPC backend.

    This class provides a unified interface for Qwen3 embedding generation,
    supporting both gRPC-based communication with isolated GPU services and
    legacy in-process model loading. The service automatically selects the
    appropriate backend based on feature flag configuration.

    Key Features:
        - Feature flag-controlled backend selection (gRPC vs in-process)
        - Comprehensive error handling with rich exception types
        - Performance monitoring and metrics collection
        - Batch processing capabilities for efficiency
        - Automatic retry and circuit breaker integration

    Attributes:
        model_name: Name of the Qwen3 model used for embeddings (e.g., "Qwen/Qwen2.5-7B-Instruct")
        embedding_dimension: Expected dimensionality of the returned vectors (default: 4096)
        batch_size: Maximum number of embeddings requested per batch (default: 8)
        max_seq_length: Maximum token length before truncation (default: 2048)
        use_grpc: Whether gRPC mode is enabled based on feature flags
        grpc_client: Instance of Qwen3GRPCClient when gRPC mode is enabled
        _legacy_client: Legacy model client for in-process mode (deprecated)

    Invariants:
        - embedding_dimension must be positive and match model capabilities
        - batch_size must be positive and reasonable for memory constraints
        - max_seq_length must be positive and appropriate for model limits

    Thread Safety:
        - Not thread-safe: Maintains mutable client state and should be used from a single thread
        - Concurrent access may lead to race conditions in model loading and inference

    Lifecycle:
        - Created with configuration parameters and optional legacy GPU manager
        - Automatically initializes appropriate backend (gRPC or legacy) based on feature flags
        - Used for embedding generation operations
        - No explicit cleanup required for gRPC mode; legacy mode may require model unloading

    Example:
        >>> service = Qwen3Service()
        >>> embeddings = service.generate_embeddings(["sample text"])
        >>> print(f"Generated embedding with {len(embeddings[0])} dimensions")

    """

    def __init__(
        self,
        model_name: str = "Qwen/Qwen2.5-7B-Instruct",
        embedding_dimension: int = 4096,
        batch_size: int = 8,
        max_seq_length: int = 2048,
        gpu_manager: Any = None,
    ) -> None:
        """Initialize the Qwen3 service.

        Args:
        ----
            model_name: Name of the Qwen3 model
            embedding_dimension: Expected embedding dimension
            batch_size: Batch size for processing
            max_seq_length: Maximum sequence length
            gpu_manager: GPU manager instance (legacy, not used in gRPC mode)

        """
        self.model_name = model_name
        self.embedding_dimension = embedding_dimension
        self.batch_size = batch_size
        self.max_seq_length = max_seq_length
        self.gpu_manager = gpu_manager

        # Get settings to determine if we should use gRPC
        settings = get_settings()

        if settings.retrieval.qwen3.use_grpc:
            # Use gRPC client for torch isolation
            self.grpc_client = Qwen3GRPCClient(
                endpoint=settings.retrieval.qwen3.grpc_endpoint,
                timeout=settings.retrieval.qwen3.grpc_timeout,
                max_retries=settings.retrieval.qwen3.grpc_max_retries,
                retry_delay=settings.retrieval.qwen3.grpc_retry_delay,
            )
            self.use_grpc = True
            self._is_loaded = True  # gRPC service is always "loaded"

            logger.info(
                "Qwen3 service initialized with gRPC client",
                extra={
                    "model_name": model_name,
                    "embedding_dimension": embedding_dimension,
                    "batch_size": batch_size,
                    "grpc_endpoint": settings.retrieval.qwen3.grpc_endpoint,
                    "grpc_timeout": settings.retrieval.qwen3.grpc_timeout,
                },
            )
        else:
            # Legacy in-process model loading (deprecated)
            self.grpc_client = None
            self.use_grpc = False
            self._is_loaded = False

            logger.warning(
                "Qwen3 service initialized in legacy in-process mode",
                extra={
                    "model_name": model_name,
                    "embedding_dimension": embedding_dimension,
                    "batch_size": batch_size,
                    "deprecation_warning": "In-process mode will be removed in future version",
                },
            )

    def _load_model(self) -> None:
        """Load the Qwen3 model (legacy in-process mode only)."""
        if self.use_grpc:
            logger.info("Model loading not required in gRPC mode")
            return

        logger.warning(
            "Attempting to load Qwen3 model in-process (deprecated)",
            extra={
                "model_name": self.model_name,
                "deprecation_warning": "In-process mode will be removed in future version",
            },
        )

        # Legacy model loading would go here
        # For now, we'll raise an error to encourage migration to gRPC
        raise NotImplementedError(
            "In-process Qwen3 model loading is deprecated. "
            "Please enable gRPC mode by setting qwen3.use_grpc=True in settings."
        )

    def _preprocess_text(self, text: str) -> str:
        """Preprocess text for embedding generation."""
        # Basic text preprocessing
        processed_text = text.strip()

        # Truncate if too long
        if len(processed_text) > self.max_seq_length * 4:  # Rough character-to-token ratio
            processed_text = processed_text[:self.max_seq_length * 4]
            logger.warning(
                "Text truncated for Qwen3 processing",
                extra={
                    "original_length": len(text),
                    "truncated_length": len(processed_text),
                    "max_seq_length": self.max_seq_length,
                },
            )

        return processed_text

    def _generate_embedding(self, text: str) -> list[float]:
        """Generate embedding for a single text (gRPC or legacy)."""
        if self.use_grpc:
            # Use gRPC client
            embeddings = self.grpc_client.embed_texts(
                texts=[text],
                model_name=self.model_name,
                namespace="default",
                normalize=True,
            )
            return embeddings[0] if embeddings else []
        else:
            # Legacy in-process implementation
            raise NotImplementedError(
                "In-process embedding generation is deprecated. "
                "Please enable gRPC mode by setting qwen3.use_grpc=True in settings."
            )

    def generate_embedding(self, chunk_id: str, contextualized_text: str) -> Qwen3Result:
        """Generate embedding for a single chunk.

        Args:
        ----
            chunk_id: Chunk identifier
            contextualized_text: Contextualized text for embedding

        Returns:
        -------
            Qwen3 embedding result

        Raises:
        ------
            Qwen3ProcessingError: When embedding generation fails
            ValueError: When input parameters are invalid

        """
        start_time = time.perf_counter()

        try:
            # Preprocess text
            processed_text = self._preprocess_text(contextualized_text)

            # Generate embedding
            embedding_vector = self._generate_embedding(processed_text)

            processing_time = time.perf_counter() - start_time

            # Get GPU memory usage if available (legacy mode only)
            gpu_memory_used_mb = 0.0
            if not self.use_grpc and self.gpu_manager and hasattr(self.gpu_manager, "get_gpu_memory_used"):
                gpu_memory_used_mb = self.gpu_manager.get_gpu_memory_used()

            result = Qwen3Result(
                chunk_id=chunk_id,
                embedding=embedding_vector,
                model_name=self.model_name,
                preprocessing_version="1.0.0",
                processing_time_seconds=processing_time,
                gpu_memory_used_mb=gpu_memory_used_mb,
            )

            # Record metrics
            QWEN3_PROCESSING_SECONDS.labels(operation="generate_embedding", status="ok").observe(
                processing_time
            )
            QWEN3_OPERATIONS_TOTAL.labels(operation="generate_embedding", status="ok").inc()
            QWEN3_EMBEDDING_DIMENSIONS.labels(operation="generate_embedding").observe(
                len(embedding_vector)
            )

            logger.info(
                "Qwen3 embedding generated",
                extra={
                    "chunk_id": chunk_id,
                    "embedding_dimension": len(embedding_vector),
                    "processing_time_seconds": processing_time,
                    "gpu_memory_used_mb": gpu_memory_used_mb,
                    "mode": "grpc" if self.use_grpc else "in_process",
                },
            )

            return result

        except Qwen3ServiceUnavailableError as e:
            processing_time = time.perf_counter() - start_time

            QWEN3_PROCESSING_SECONDS.labels(operation="generate_embedding", status="error").observe(
                processing_time
            )
            QWEN3_OPERATIONS_TOTAL.labels(operation="generate_embedding", status="error").inc()

            logger.error(
                "Qwen3 gRPC service unavailable",
                extra={
                    "chunk_id": chunk_id,
                    "error": str(e),
                    "processing_time_seconds": processing_time,
                    "endpoint": e.endpoint,
                },
            )
            raise Qwen3ProcessingError(f"Qwen3 service unavailable: {e}") from e

        except Exception as e:
            processing_time = time.perf_counter() - start_time

            QWEN3_PROCESSING_SECONDS.labels(operation="generate_embedding", status="error").observe(
                processing_time
            )
            QWEN3_OPERATIONS_TOTAL.labels(operation="generate_embedding", status="error").inc()

            logger.error(
                "Failed to generate Qwen3 embedding",
                extra={
                    "chunk_id": chunk_id,
                    "error": str(e),
                    "processing_time_seconds": processing_time,
                    "mode": "grpc" if self.use_grpc else "in_process",
                },
            )
            raise Qwen3ProcessingError(f"Failed to generate embedding: {e}") from e

    def generate_embeddings_batch(self, chunks: list[tuple[str, str]]) -> list[Qwen3Result]:
        """Generate embeddings for multiple chunks in batch.

        Args:
        ----
            chunks: List of (chunk_id, contextualized_text) tuples

        Returns:
        -------
            List of Qwen3 embedding results

        Raises:
        ------
            Qwen3ProcessingError: When batch embedding generation fails
            ValueError: When input chunks are invalid or empty

        """
        start_time = time.perf_counter()

        try:
            if not self._is_loaded:
                self._load_model()

            results = []

            if self.use_grpc:
                # Use gRPC client for batch processing
                texts = [self._preprocess_text(contextualized_text) for _, contextualized_text in chunks]

                embeddings = self.grpc_client.embed_texts(
                    texts=texts,
                    model_name=self.model_name,
                    namespace="default",
                    normalize=True,
                )

                # Create results
                for i, (chunk_id, _) in enumerate(chunks):
                    embedding_vector = embeddings[i] if i < len(embeddings) else []

                    result = Qwen3Result(
                        chunk_id=chunk_id,
                        embedding=embedding_vector,
                        model_name=self.model_name,
                        preprocessing_version="1.0.0",
                        processing_time_seconds=0.0,  # Will be updated below
                        gpu_memory_used_mb=0.0,
                    )
                    results.append(result)
            else:
                # Legacy in-process batch processing
                for i in range(0, len(chunks), self.batch_size):
                    batch_chunks = chunks[i : i + self.batch_size]

                    # Generate embeddings for batch
                    batch_results = []
                    for chunk_id, contextualized_text in batch_chunks:
                        result = self.generate_embedding(chunk_id, contextualized_text)
                        batch_results.append(result)

                    results.extend(batch_results)

            processing_time = time.perf_counter() - start_time

            # Update processing times for batch results
            for result in results:
                result.processing_time_seconds = processing_time / len(results)

            QWEN3_PROCESSING_SECONDS.labels(
                operation="generate_embeddings_batch", status="ok"
            ).observe(processing_time)
            QWEN3_OPERATIONS_TOTAL.labels(operation="generate_embeddings_batch", status="ok").inc()
            QWEN3_BATCH_SIZE.labels(operation="generate_embeddings_batch").observe(len(chunks))

            logger.info(
                "Qwen3 embeddings generated in batch",
                extra={
                    "chunk_count": len(chunks),
                    "processing_time_seconds": processing_time,
                    "mode": "grpc" if self.use_grpc else "in_process",
                },
            )

            return results

        except Qwen3ServiceUnavailableError as e:
            processing_time = time.perf_counter() - start_time

            QWEN3_PROCESSING_SECONDS.labels(
                operation="generate_embeddings_batch", status="error"
            ).observe(processing_time)
            QWEN3_OPERATIONS_TOTAL.labels(
                operation="generate_embeddings_batch", status="error"
            ).inc()

            logger.error(
                "Qwen3 gRPC service unavailable for batch processing",
                extra={
                    "chunk_count": len(chunks),
                    "error": str(e),
                    "processing_time_seconds": processing_time,
                    "endpoint": e.endpoint,
                },
            )
            raise Qwen3ProcessingError(f"Qwen3 service unavailable: {e}") from e

        except Exception as e:
            processing_time = time.perf_counter() - start_time

            QWEN3_PROCESSING_SECONDS.labels(
                operation="generate_embeddings_batch", status="error"
            ).observe(processing_time)
            QWEN3_OPERATIONS_TOTAL.labels(
                operation="generate_embeddings_batch", status="error"
            ).inc()

            logger.error(
                "Failed to generate Qwen3 embeddings in batch",
                extra={
                    "chunk_count": len(chunks),
                    "error": str(e),
                    "processing_time_seconds": processing_time,
                    "mode": "grpc" if self.use_grpc else "in_process",
                },
            )
            raise Qwen3ProcessingError(f"Failed to generate embeddings in batch: {e}") from e

    def health_check(self) -> dict[str, Any]:
        """Check Qwen3 service health.

        Returns
        -------
            Health status information

        Raises:
        ------
            Qwen3ServiceUnavailableError: When health check fails

        """
        try:
            if self.use_grpc:
                # Use gRPC client health check
                health_status = self.grpc_client.health_check()
                health_status.update({
                    "model_name": self.model_name,
                    "embedding_dimension": self.embedding_dimension,
                    "batch_size": self.batch_size,
                    "max_seq_length": self.max_seq_length,
                    "mode": "grpc",
                })
                return health_status
            else:
                # Legacy in-process health check
                # Check GPU availability
                gpu_available = False
                if self.gpu_manager and hasattr(self.gpu_manager, "is_gpu_available"):
                    gpu_available = self.gpu_manager.is_gpu_available()

                # Check model loading
                model_loaded = self._is_loaded

                # Try to load model if not loaded
                if not model_loaded:
                    try:
                        self._load_model()
                        model_loaded = True
                    except Exception as e:
                        logger.warning(
                            "Qwen3 model failed to load during health check", extra={"error": str(e)}
                        )

                health_status = {
                    "status": "healthy" if gpu_available and model_loaded else "unhealthy",
                    "gpu_available": gpu_available,
                    "model_loaded": model_loaded,
                    "model_name": self.model_name,
                    "embedding_dimension": self.embedding_dimension,
                    "batch_size": self.batch_size,
                    "max_seq_length": self.max_seq_length,
                    "mode": "in_process",
                }

                return health_status

        except Exception as e:
            logger.error("Qwen3 health check failed", extra={"error": str(e)})
            return {
                "status": "unhealthy",
                "error": str(e),
                "model_name": self.model_name,
                "mode": "grpc" if self.use_grpc else "in_process",
            }

    def get_service_stats(self) -> dict[str, Any]:
        """Get Qwen3 service statistics.

        Returns
        -------
            Dictionary with service statistics

        Raises:
        ------
            None

        """
        if self.use_grpc:
            # Get gRPC service info
            service_info = self.grpc_client.get_service_info()
            service_info.update({
                "model_name": self.model_name,
                "embedding_dimension": self.embedding_dimension,
                "batch_size": self.batch_size,
                "max_seq_length": self.max_seq_length,
                "mode": "grpc",
            })
            return service_info
        else:
            # Legacy in-process stats
            return {
                "model_name": self.model_name,
                "embedding_dimension": self.embedding_dimension,
                "batch_size": self.batch_size,
                "max_seq_length": self.max_seq_length,
                "model_loaded": self._is_loaded,
                "gpu_available": (
                    self.gpu_manager.is_gpu_available()
                    if self.gpu_manager and hasattr(self.gpu_manager, "is_gpu_available")
                    else False
                ),
                "mode": "in_process",
            }

    def close(self) -> None:
        """Close the service and clean up resources."""
        if self.use_grpc and self.grpc_client:
            self.grpc_client.close()
            logger.info("Qwen3 gRPC client closed")

    def __enter__(self) -> Qwen3Service:
        """Context manager entry."""
        return self

    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:
        """Context manager exit."""
        self.close()


# ==============================================================================
# EXPORTS
# ==============================================================================

__all__ = [
    "Qwen3Service",
    "Qwen3Result",
    "Qwen3ProcessingError",
]
