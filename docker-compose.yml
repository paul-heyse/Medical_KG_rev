version: "3.8"

services:
    zookeeper:
        image: confluentinc/cp-zookeeper:7.5.0
        environment:
            ZOOKEEPER_CLIENT_PORT: 2181
            ZOOKEEPER_TICK_TIME: 2000
        ports:
            - "2181:2181"
        healthcheck:
            test: ["CMD-SHELL", "zookeeper-shell localhost:2181 ls /"]
            interval: 10s
            timeout: 5s
            retries: 5

    kafka:
        image: confluentinc/cp-kafka:7.5.0
        depends_on:
            - zookeeper
        ports:
            - "9092:9092"
        environment:
            KAFKA_BROKER_ID: 1
            KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
            KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
            KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
            KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
            KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
            KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
        healthcheck:
            test:
                [
                    "CMD",
                    "kafka-topics",
                    "--bootstrap-server",
                    "localhost:9092",
                    "--list",
                ]
            interval: 10s
            timeout: 10s
            retries: 5

    neo4j:
        image: neo4j:5.12
        environment:
            NEO4J_AUTH: neo4j/testpassword
            NEO4JLABS_PLUGINS: '["apoc"]'
        ports:
            - "7474:7474"
            - "7687:7687"
        volumes:
            - neo4j-data:/data
        healthcheck:
            test: ["CMD-SHELL", "neo4j status | grep 'Running'"]
            interval: 15s
            timeout: 10s
            retries: 5

    opensearch:
        image: opensearchproject/opensearch:2.11.0
        environment:
            discovery.type: single-node
            plugins.security.disabled: "true"
            OPENSEARCH_JAVA_OPTS: "-Xms512m -Xmx512m"
        ports:
            - "9200:9200"
            - "9600:9600"
        healthcheck:
            test: ["CMD-SHELL", "curl -sSf http://localhost:9200 >/dev/null"]
            interval: 15s
            timeout: 10s
            retries: 5

    vllm-server:
        image: vllm/vllm-openai:v0.11.0
        command:
            - --model
            - opendatalab/MinerU2.5-2509-1.2B
            - --host
            - "0.0.0.0"
            - --port
            - "8000"
            - --gpu-memory-utilization
            - "0.75"
            - --max-model-len
            - "8192"
            - --tensor-parallel-size
            - "1"
            - --dtype
            - auto
            - --trust-remote-code
        deploy:
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          count: 1
                          capabilities: [gpu]
        ports:
            - "8000:8000"
        volumes:
            - ~/.cache/huggingface:/root/.cache/huggingface:ro
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
            interval: 30s
            timeout: 10s
            retries: 3
            start_period: 120s
        ipc: host
        ulimits:
            memlock: -1
            stack: 67108864
        networks:
            - medical-kg-net

    mineru-worker:
        build:
            context: .
            dockerfile: ops/docker/Dockerfile.mineru-worker
        image: medical-kg/mineru-worker:latest
        environment:
            MK_MINERU__VLLM_SERVER__BASE_URL: http://vllm-server:8000
            MK_MINERU__WORKERS__BACKEND: vlm-http-client
            MK_MINERU__HTTP_CLIENT__CONNECTION_POOL_SIZE: "10"
            MK_MINERU__HTTP_CLIENT__KEEPALIVE_CONNECTIONS: "5"
            MK_MINERU__HTTP_CLIENT__RETRY_ATTEMPTS: "3"
            MK_DOCLING_VLM__MODEL_PATH: /models/gemma3-12b
            MK_DOCLING_VLM__GPU_MEMORY_FRACTION: "0.95"
            MK_DOCLING_VLM__MAX_MODEL_LEN: "4096"
            MK_DOCLING_VLM__RETRY_ATTEMPTS: "3"
            MK_FEATURE_FLAGS__PDF_PROCESSING_BACKEND: docling_vlm
            PYTHONPATH: /app
        depends_on:
            vllm-server:
                condition: service_healthy
        networks:
            - medical-kg-net
        healthcheck:
            test:
                ["CMD", "python", "-c", "import Medical_KG_rev.services.mineru"]
            interval: 30s
            timeout: 10s
            retries: 3
        volumes:
            - gemma3-cache:/models/gemma3-12b

    vllm-qwen3:
        build:
            context: .
            dockerfile: ops/vllm/Dockerfile.qwen3-embedding-8b
        ports:
            - "8001:8001"
        environment:
            - CUDA_VISIBLE_DEVICES=0
        deploy:
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          count: 1
                          capabilities: [gpu]
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
            interval: 30s
            timeout: 10s
            retries: 3

volumes:
    neo4j-data:
    gemma3-cache:

networks:
    default:
        name: medical-kg-network
    medical-kg-net:
        name: medical-kg-network
        driver: bridge
